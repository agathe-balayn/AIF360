{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring and characterizing fairness as a notion of inclusiveness.\n",
    "\n",
    "Certain Machine Learning models are made to perform classification tasks of samples over labels which are subjective, what means that several users of the models might judge the label of the sample differently depending on their personal experience.\n",
    "\n",
    "The predictions of the models might contain biases towards certain types of judgements which are more common than others and consequently easier to learn, and ignore other judgements. These biases might already be contained in the training dataset or generated by the classification model. \n",
    "\n",
    "However for the predictions to be fair towards each user of the model, they should be inclusive of all the different judgements, and possibly should be tuned to each of the users.\n",
    "\n",
    "In this tutorial we teach:\n",
    "- how to use metrics to measure how fair according to this notion of inclusiveness the models are,\n",
    "- and how to use various characterizations of the predictions to understand where the unfairness might come from.\n",
    "\n",
    "The tutorial is based on the example use-case of a Machine Learning model to classify the toxicity of a sentence (see image below).\n",
    "We train a classifier (Logistic Regression) using the toxicity dataset (sentences and toxicity labels) to predict sentence toxicity, and evaluate how fair the outputs of the process are based on the ground truth annotations provided by multiple judges (crowdsourcing annotators).\n",
    "\n",
    "![title](images/overview_tutorial_fairness_inclusiveness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  \n",
    "\n",
    "import os\n",
    "\n",
    "from aif360.datasets import ToxicityDataset\n",
    "from aif360.metrics import InclusivenessLabelDatasetMetric\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as sk_met\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example to load the full toxicity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "import copy\n",
    "### The toxicity dataset (toxicity_annotations.tsv, toxicity_annotated_comments.tsv, toxicity_worker_demographics.tsv) should be downloaded from https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973\n",
    "### and placed in the folder \"data/raw/toxicity\".\n",
    "# Example on how to load the full dataset.\n",
    "#tox_dataset = ToxicityDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def clean_data(annotations, worker_demo, comments):\n",
    "        \n",
    "        # Preprocess workers.\n",
    "        worker_demo = worker_demo.replace(np.NaN, 'nan')\n",
    " \n",
    "        #### Add all the information to the annotations.\n",
    "        # Add the worker demographics.\n",
    "        annotations = annotations.reset_index().merge(worker_demo, on='worker_id', how='left').set_index(annotations.index.names)\n",
    "        # Remove the unknown demographics and the demographics with a NaN. And put them in a general test set.\n",
    "        annotations = annotations.replace(np.NaN, 'nan')\n",
    "        annotations.loc[((annotations['english_first_language'].str.contains('nan')) |(annotations['gender'].str.contains('nan')) | (annotations['age_group'].str.contains('nan')) | (annotations['education'].str.contains('nan')) ),'general_split'] = 1 #'test'\n",
    "        annotations = annotations.reset_index()\n",
    "        annotations['english_first_language'].replace('nan', 2, inplace=True)\n",
    "        annotations['pop_label'] = annotations.loc[:, ['gender', 'age_group', 'education']].apply(lambda x: ' '.join([str(x['gender']),str(x['age_group']), str(x['education'])]), axis=1)    \n",
    "          \n",
    "        # Add the comments in order to train / test the ML models.\n",
    "        annotations = annotations.reset_index().merge(comments.loc[:, ['comment']].reset_index(), on='rev_id', how='left').set_index('index')\n",
    "        return annotations\n",
    "    \n",
    "def normalize_text(text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        stopword_set = set(stopwords.words('english'))\n",
    "        stemmer = PorterStemmer()\n",
    "        # Convert text to lower-case and strip punctuation/symbols from words.\n",
    "        norm_text = text.lower()\n",
    "        # Replace breaks with spaces.\n",
    "        norm_text = norm_text.replace('<br />', ' ')\n",
    "        # Pad punctuation with spaces on both sides.\n",
    "        for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "            norm_text = norm_text.replace(char, ' ' + char + ' ') \n",
    "        # Tokenize.\n",
    "        norm_text = tokenizer.tokenize(norm_text)\n",
    "        # Remove stop words.\n",
    "        norm_text = [w for w in norm_text if not w in stopword_set]\n",
    "        norm_text = \" \".join(norm_text)\n",
    "        return norm_text\n",
    "\n",
    "    \n",
    "def clean_comments(comments):\n",
    "        comments['comment'] = comments.loc[:, 'comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "        comments['comment'] = comments.loc[:, 'comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "        comments['comment'] = comments.loc[:, 'comment'].apply(lambda x: normalize_text(x))\n",
    "        return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the documents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the dataset.\n",
      "C:\\Users\\AgatheBalayn\\Documents\\thesis_related\\AIF360\\examples\\..\\aif360\\data\\raw\\toxicity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Load the dataset.\")\n",
    "    filepath = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), '..', 'aif360', 'data', 'raw', 'toxicity')\n",
    "    print(filepath)\n",
    "    comments = pd.read_csv(filepath + '/toxicity_annotated_comments.tsv', sep = '\\t', dtype = {'rev_id':int, 'comment':str}, index_col = 0)\n",
    "    annotations = pd.read_csv(filepath + '/toxicity_annotations.tsv',  sep = '\\t', index_col = 0)\n",
    "    worker_demo = pd.read_csv(filepath + '/toxicity_worker_demographics.tsv', sep='\\t')\n",
    "except IOError as err:\n",
    "    print(\"IOError: {}\".format(err))\n",
    "    print(\"To use this class, please download the following files from https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973:\")\n",
    "    print(\"\\n\\ttoxicity_annotated_comments.tsv\")\n",
    "    print(\"\\ttoxicity_annotations.tsv\")\n",
    "    print(\"\\ttoxicity_worker_demographics.tsv\")\n",
    "    print(\"\\nand place them, as-is, in the folder:\")\n",
    "    print(\"\\n\\t{}\\n\".format(os.path.abspath(os.path.join(os.path.abspath(__file__), '..', '..', 'data', 'raw', 'toxicity'))))\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For now we do not use the whole dataset to be faster.\n",
    "n_lim = 0\n",
    "comments = clean_comments(comments)\n",
    "if n_lim > 0:\n",
    "    annotations = annotations.iloc[0:n_lim]\n",
    "# Merge the different datasets.\n",
    "annotations = clean_data(annotations, worker_demo, comments)\n",
    "# Compute the ground truth (majority vote) label.\n",
    "annotations['MV'] = annotations.groupby(['rev_id'])['toxicity'].transform(lambda x : (x.mean() >= 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the unique comments for training and testing of the ML models.\n",
    "def prepare_aggregated_data(comment_):\n",
    "    # Get the unique comments.\n",
    "    comment_ = comment_.drop_duplicates('rev_id')\n",
    "    # Cleaning.\n",
    "    return comment_.loc[:, ['comment', 'MV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Training and test sets with annotations as ground truth:\n",
    "annotations_train, annotations_test = train_test_split(annotations, test_size=0.3)\n",
    "# To ensure they are copies and not views of annotations:\n",
    "annotations_train = annotations_train.copy()\n",
    "annotations_test = annotations_test.copy()\n",
    "# Training and test sets with majority-vote as ground truth:\n",
    "comments_train = prepare_aggregated_data(annotations_train.copy())\n",
    "comments_test = prepare_aggregated_data(annotations_test.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load the model.\n",
    "class DataFrameColumnExtracter_doc(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column].values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The ML model is trained on the majority vote labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('sentences_features', Pipeline(memory=None,\n",
       "     steps=[('sentence_extractor', DataFrameColumnExtracter_doc(column='comment')), ('vect', CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lower...alty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.01, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model, here Logistic Regression model.\n",
    "clf_LR = Pipeline([# Sentences.\n",
    "                  ('sentences_features', Pipeline([\n",
    "                      ('sentence_extractor', DataFrameColumnExtracter_doc('comment')),#.values.astype('U'),\n",
    "                    ('vect', CountVectorizer(max_features = 1500, ngram_range = (1,5), analyzer = 'char')),\n",
    "                     ('tf', TfidfTransformer(norm = 'l2'))\n",
    "                  ])),\n",
    "            # Classifier.\n",
    "            ('clf', LogisticRegression(solver='liblinear'))#C=LR_C, tol=LR_C_tol))\n",
    "        ])\n",
    "## 1) Perform grid search over the parameters of the model.\n",
    "\n",
    "# Parameters of the grid search:\n",
    "tuned_parameters = {'clf__C': [1e-4, 1e-2, 1, 10], 'clf__tol': [1, 1e-2, 1e-4]} \n",
    "\n",
    "# Initialize the grid search.\n",
    "clf = GridSearchCV(clf_LR, tuned_parameters, cv=5, verbose=0)\n",
    "\n",
    "# Functions to save and load the results of the grid search.\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "# Use a small number of data to train the model faster (this is used for quick testing).\n",
    "nb_data = 10000\n",
    "\n",
    "# Train the grid search.\n",
    "# To comment if already run ones and the paramters were saved.\n",
    "# =============\n",
    "#best_model = clf.fit(comments_train[0:nb_data], comments_train['MV'][0:nb_data])\n",
    "#best_parameters = best_model.best_params_  \n",
    "#print(best_parameters)  \n",
    "#best_result = best_model.best_score_  \n",
    "#print(best_result)  \n",
    "#save_obj(best_parameters, 'best_param_LR_aggregated')\n",
    "# =============\n",
    "\n",
    "\n",
    "## 2) Train the final model.\n",
    "best_parameters = load_obj('best_param_LR_aggregated')\n",
    "clf_LR.set_params(**best_parameters)\n",
    "clf_LR.fit(comments_train.iloc[0:nb_data], comments_train.iloc[0:nb_data, comments_train.columns.get_loc('MV')])\n",
    "\n",
    "\n",
    "## 3) Evaluate general performance.\n",
    "#train_pred = clf_LR.predict(comments_train.iloc[0:nb_data])\n",
    "#test_pred = clf_LR.predict(comments_test.iloc[0:nb_data])\n",
    "#print(\"Training accuracy: \", sk_met.accuracy_score(comments_train.iloc[0:nb_data, comments_train.columns.get_loc('MV')], train_pred))\n",
    "#print(\"Test accuracy: \", sk_met.accuracy_score(comments_test.iloc[0:nb_data, comments_test.columns.get_loc('MV')], test_pred))\n",
    "\n",
    "#C_train = sk_met.confusion_matrix(comments_train.iloc[0:nb_data, comments_train.columns.get_loc('MV')], train_pred)\n",
    "#C_train = C_train / C_train.astype(np.float).sum(axis=0)\n",
    "#C_test = sk_met.confusion_matrix(comments_test.iloc[0:nb_data, comments_test.columns.get_loc('MV')], test_pred)\n",
    "#C_test = C_test / C_test.astype(np.float).sum(axis=0)\n",
    "#print(\"Training confusion matrix:\", C_train)\n",
    "#print(\"Test confusion matrix:\", C_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a second ML model on the annotation (not majority-vote) labels in order to compare the fairness measures later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('sentences_features', Pipeline(memory=None,\n",
       "     steps=[('sentence_extractor', DataFrameColumnExtracter_doc(column='comment')), ('vect', CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lower...alty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.01, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model, here Logistic Regression model.\n",
    "clf_LR_annotations = Pipeline([# Sentences.\n",
    "                  ('sentences_features', Pipeline([\n",
    "                      ('sentence_extractor', DataFrameColumnExtracter_doc('comment')),#.values.astype('U'),\n",
    "                    ('vect', CountVectorizer(max_features = 1500, ngram_range = (1,5), analyzer = 'char')),\n",
    "                     ('tf', TfidfTransformer(norm = 'l2'))\n",
    "                  ])),\n",
    "            # Classifier.\n",
    "            ('clf', LogisticRegression(solver='liblinear'))#C=LR_C, tol=LR_C_tol))\n",
    "        ])\n",
    "\n",
    "## 1) Perform grid search over the parameters of the model.\n",
    "\n",
    "# Parameters of the grid search:\n",
    "tuned_parameters = {'clf__C': [1e-4, 1e-2, 1, 10], 'clf__tol': [1, 1e-2, 1e-4]} \n",
    "\n",
    "# Initialize the grid search.\n",
    "clf_annotations = GridSearchCV(clf_LR_annotations, tuned_parameters, cv=5, verbose=0)\n",
    "    \n",
    "# Use a small number of data to train the model faster.\n",
    "nb_data = 10000\n",
    "\n",
    "# Train the grid search.\n",
    "# To comment if already run ones and the paramters were saved.\n",
    "# =============\n",
    "#best_model = clf_annotations.fit(annotations_train[0:nb_data], annotations_train['toxicity'][0:nb_data])\n",
    "#best_parameters = best_model.best_params_  \n",
    "#print(best_parameters)  \n",
    "#best_result = best_model.best_score_  \n",
    "#print(best_result)  \n",
    "#save_obj(best_parameters, 'best_param_LR_annotations')\n",
    "# =============\n",
    "\n",
    "\n",
    "## 2) Train the final model.\n",
    "best_parameters = load_obj('best_param_LR_annotations')\n",
    "clf_LR_annotations.set_params(**best_parameters)\n",
    "clf_LR_annotations.fit(annotations_train.iloc[0:nb_data], annotations_train.iloc[0:nb_data, annotations_train.columns.get_loc('toxicity')])\n",
    "\n",
    "\n",
    "## 3) Evaluate general performance.\n",
    "#train_pred = clf_LR_annotations.predict(annotations_train.iloc[0:nb_data])\n",
    "#test_pred = clf_LR_annotations.predict(annotations_test.iloc[0:nb_data])\n",
    "#print(\"Training accuracy: \", sk_met.accuracy_score(annotations_train.iloc[0:nb_data, annotations_train.columns.get_loc('toxicity')], train_pred))\n",
    "#print(\"Test accuracy: \", sk_met.accuracy_score(annotations_test.iloc[0:nb_data, annotations_test.columns.get_loc('toxicity')], test_pred))\n",
    "\n",
    "#C_train = sk_met.confusion_matrix(annotations_train.iloc[0:nb_data, annotations_train.columns.get_loc('toxicity')], train_pred)\n",
    "#C_train = C_train / C_train.astype(np.float).sum(axis=0)\n",
    "#C_test = sk_met.confusion_matrix(annotations_test.iloc[0:nb_data, annotations_test.columns.get_loc('toxicity')], test_pred)\n",
    "#C_test = C_test / C_test.astype(np.float).sum(axis=0)\n",
    "#print(\"Training confusion matrix:\", C_train)\n",
    "#print(\"Test confusion matrix:\", C_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove variables not used anymore to empty memory\n",
    "del annotations, comments, worker_demo, comments_train, comments_test #, train_pred, test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the fairness performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import SubjectivityDataset\n",
    "\n",
    "def default_preprocessing(df):\n",
    "    return df\n",
    "\n",
    "# Wrapper to load the datasets to compute the fairness on.\n",
    "def subjectivity_dataset_wrapper(annotations, label_name,\n",
    "                 protected_attribute_names=['gender', 'english_first_language', 'age_group', 'education', 'rev_id', 'worker_id', 'pop_label'], privileged_classes=None,\n",
    "                 instance_weights_name=None, categorical_features=[],\n",
    "                 features_to_keep=[], features_to_drop=[], na_values=[],\n",
    "                 custom_preprocessing=default_preprocessing, \n",
    "                metadata={'label_maps': [{1.0: 'Toxic', 0.0: 'Non-toxic'}],},\n",
    "                mapping_categorical_protected=(('gender',('female','male', 'other', 'nan')), ('age_group',('Under 18', '18-30', '30-45', '45-60', 'Over 60', 'nan')), ('education',('none', 'hs', 'some', 'bachelors', 'masters', 'professional', 'doctorate', 'nan')))):\n",
    "    \n",
    "    if 'comment' in annotations.columns.tolist():\n",
    "        annotations.drop('comment', axis=1, inplace=True)\n",
    "    if 'general_split' in annotations.columns.tolist():\n",
    "        annotations.drop('general_split', axis=1, inplace=True)\n",
    "          \n",
    "    # Create the ground truth data.\n",
    "    if 'toxicity' in annotations.columns.tolist():\n",
    "        annotations['GT'] = annotations['toxicity'].copy()\n",
    "    if label_name != 'toxicity':\n",
    "        # Delete the 'toxicity' column.\n",
    "        if 'toxicity' in annotations.columns.tolist():\n",
    "            annotations.drop('toxicity', axis=1, inplace=True)\n",
    "        annotations.rename(columns={label_name:'toxicity'}, inplace=True)\n",
    "        label_name = 'toxicity'\n",
    "    else:\n",
    "        if 'pred_1' in annotations.columns.tolist():\n",
    "            annotations.drop('pred_1', axis=1, inplace=True)\n",
    "    annotations = annotations.astype({\"toxicity\": float})\n",
    "\n",
    "    # Make the categorical data numbers.\n",
    "    if mapping_categorical_protected != ():\n",
    "        for tuple_type in mapping_categorical_protected:\n",
    "            for tuple_details in tuple_type:\n",
    "                if tuple_type.index(tuple_details) == 0:\n",
    "                    key = tuple_details\n",
    "                else:\n",
    "                    for tuple_categories in tuple_details:\n",
    "                        annotations[key].replace(tuple_categories, tuple_details.index(tuple_categories), inplace=True)\n",
    "\n",
    "    annotations['pop_label'] = annotations[['gender', 'age_group', 'education']].apply(lambda x: int(''.join([str(x['gender']),str(x['age_group']), str(x['education'])])), axis=1)    \n",
    "    annotations = annotations.loc[:, ['rev_id', 'worker_id', 'toxicity', 'toxicity_score', 'gender', 'english_first_language', 'age_group', 'education', 'pop_label', 'GT']]#, 'MV']]\n",
    "    \n",
    "    dataset = SubjectivityDataset(annotations, label_name, 'GT',\n",
    "                 protected_attribute_names=protected_attribute_names, privileged_classes=privileged_classes,\n",
    "                 instance_weights_name=instance_weights_name, categorical_features=categorical_features,\n",
    "                 features_to_keep=features_to_keep, features_to_drop=features_to_drop, na_values=na_values,\n",
    "                 custom_preprocessing=custom_preprocessing, metadata=metadata)\n",
    "    return dataset, annotations\n",
    "\n",
    "\n",
    "# The dataset might be too large for the classifier to process all the data. In this case, it is splitted to get the predictions on all the data.\n",
    "def compute_pred_dataset(classifier, dataset_orig_train, prediction_col, nb_data):\n",
    "    for i in range(int(len(dataset_orig_train) / nb_data)):\n",
    "        low_interval = i*nb_data\n",
    "        high_interval = (i+1)*nb_data\n",
    "        dataset_orig_train.iloc[low_interval:high_interval, dataset_orig_train.columns.get_loc(prediction_col)] = classifier.predict(dataset_orig_train.iloc[low_interval:high_interval])\n",
    "    if high_interval < len(dataset_orig_train):\n",
    "        dataset_orig_train.iloc[high_interval:, dataset_orig_train.columns.get_loc(prediction_col)] = classifier.predict(dataset_orig_train.iloc[high_interval:])\n",
    "    return dataset_orig_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create SubjectivityDataset datasets out of the training and test results and ground truth on the classifier trained on the MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    940\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m             not_indexed_same=mutated or self.mutated)\n\u001b[0m\u001b[0;32m    942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_wrap_applied_output\u001b[1;34m(self, keys, values, not_indexed_same)\u001b[0m\n\u001b[0;32m   4220\u001b[0m             return self._concat_objects(keys, values,\n\u001b[1;32m-> 4221\u001b[1;33m                                         not_indexed_same=not_indexed_same)\n\u001b[0m\u001b[0;32m   4222\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupings\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_concat_objects\u001b[1;34m(self, keys, values, not_indexed_same)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnot_indexed_same\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                 copy=self.copy)\n\u001b[0m\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5420\u001b[0m             b = make_block(\n\u001b[1;32m-> 5421\u001b[1;33m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5422\u001b[0m                 placement=placement)\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5561\u001b[1;33m     \u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_empty_dtype_and_na\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget_empty_dtype_and_na\u001b[1;34m(join_units)\u001b[0m\n\u001b[0;32m   5480\u001b[0m     \u001b[0mhas_none_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5481\u001b[1;33m     \u001b[0mdtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5482\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-51c021c5abb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mannotations_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_pred_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_LR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msubjectivity_dataset_test_GT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations_subjectivity_dataset_test_GT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubjectivity_dataset_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotations_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'toxicity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0msubjectivity_dataset_test_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations_subjectivity_dataset_test_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubjectivity_dataset_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotations_test_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-62a34a00ad99>\u001b[0m in \u001b[0;36msubjectivity_dataset_wrapper\u001b[1;34m(annotations, label_name, protected_attribute_names, privileged_classes, instance_weights_name, categorical_features, features_to_keep, features_to_drop, na_values, custom_preprocessing, metadata, mapping_categorical_protected)\u001b[0m\n\u001b[0;32m     49\u001b[0m                  \u001b[0minstance_weights_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstance_weights_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                  \u001b[0mfeatures_to_keep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures_to_keep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_to_drop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures_to_drop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                  custom_preprocessing=custom_preprocessing, metadata=metadata)\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\thesis_related\\AIF360\\aif360\\datasets\\subjectivity_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, df, label_name, ground_truth_name, protected_attribute_names, privileged_classes, instance_weights_name, categorical_features, features_to_keep, features_to_drop, na_values, custom_preprocessing, metadata)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Compute the different scores later used for computing the fairness metrics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m#print(\"Compute the scores describing the dataset, used for the evaluation metric.\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mground_truth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mprotected_attribute_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprotected_attribute_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'sample_agreement'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'annotator_ADR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'popularity_percentage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\thesis_related\\AIF360\\aif360\\datasets\\subjectivity_dataset.py\u001b[0m in \u001b[0;36mcompute_scores\u001b[1;34m(self, annotations, label_name)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m## Compute the annotator average disagreement with the majority vote.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m## Compute the annotations' popularity among the other annotations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mannotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rev_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_sample_agreement\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;31m# Get the majority vote associated to each sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mannotations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MV'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rev_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0m_group_selection_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0m\u001b[0;32m    936\u001b[0m                                                    self.axis)\n\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_selected_obj\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selection\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_group_selection\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_group_selection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2680\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2682\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2683\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2726\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2727\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take\u001b[1;34m(self, indices, axis, is_copy)\u001b[0m\n\u001b[0;32m   2787\u001b[0m         new_data = self._data.take(indices,\n\u001b[0;32m   2788\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2789\u001b[1;33m                                    verify=True)\n\u001b[0m\u001b[0;32m   2790\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   4537\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4538\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[1;32m-> 4539\u001b[1;33m                                     axis=axis, allow_dups=True)\n\u001b[0m\u001b[0;32m   4540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4541\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   4419\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4420\u001b[0m             new_blocks = self._slice_take_blocks_ax0(indexer,\n\u001b[1;32m-> 4421\u001b[1;33m                                                      fill_tuple=(fill_value,))\n\u001b[0m\u001b[0;32m   4422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4423\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_tuple)\u001b[0m\n\u001b[0;32m   4499\u001b[0m                     blocks.append(blk.take_nd(blklocs[mgr_locs.indexer],\n\u001b[0;32m   4500\u001b[0m                                               \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4501\u001b[1;33m                                               fill_tuple=None))\n\u001b[0m\u001b[0;32m   4502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4503\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m             new_values = algos.take_nd(values, indexer, axis=axis,\n\u001b[1;32m-> 1254\u001b[1;33m                                        allow_fill=False)\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[0;32m   1653\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1655\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1657\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prediction_col = \"pred_1\"\n",
    "annotations_train.loc[:, prediction_col] = -1\n",
    "annotations_test.loc[:, prediction_col] = -1\n",
    "nb_data = 5000 # Here we limit the number of data for faster evaluation (however the more data, the more the evaluation is accurate).\n",
    "\n",
    "annotations_test_pred = compute_pred_dataset(clf_LR, annotations_test, prediction_col, nb_data)\n",
    "subjectivity_dataset_test_GT, annotations_subjectivity_dataset_test_GT = subjectivity_dataset_wrapper(annotations_test.copy(), 'toxicity')\n",
    "subjectivity_dataset_test_outputs, annotations_subjectivity_dataset_test_outputs = subjectivity_dataset_wrapper(annotations_test_pred.copy(), prediction_col)\n",
    "\n",
    "# Instantiate the fairness metric class.\n",
    "test_metric_inclusion = InclusivenessLabelDatasetMetric(subjectivity_dataset_test_GT, subjectivity_dataset_test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create SubjectivityDataset datasets out of the training and test results and ground truth on the classifier trained on the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_test_pred_2 = compute_pred_dataset(clf_LR_annotations, annotations_test, prediction_col, nb_data)\n",
    "subjectivity_dataset_test_outputs_2, annotations_subjectivity_dataset_test_outputs_2 = subjectivity_dataset_wrapper(annotations_test_pred.copy(), prediction_col)\n",
    "\n",
    "# Instantiate the fairness metric class.\n",
    "test_metric_inclusion_2 = InclusivenessLabelDatasetMetric(subjectivity_dataset_test_GT, subjectivity_dataset_test_outputs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty some memory\n",
    "del clf_LR, annotations_test, annotations_test_pred, subjectivity_dataset_test_outputs, annotations_subjectivity_dataset_test_outputs\n",
    "del clf_LR_annotations, annotations_test_pred_2, subjectivity_dataset_test_outputs_2, annotations_subjectivity_dataset_test_outputs_2\n",
    "del clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of the computation of different metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the classifier trained on the majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test_annotator_acc = test_metric_inclusion.average_accuracy('annotator_disagreement', number_bins=None, filtering_value=None)\n",
    "results_test_annotator_TPR = test_metric_inclusion.true_positive_rate('annotator_disagreement', number_bins=None, filtering_value=None)\n",
    "results_test_annotator_TNR = test_metric_inclusion.true_negative_rate('annotator_disagreement', number_bins=None, filtering_value=None)\n",
    "results_test_annotator_FPR = test_metric_inclusion.false_positive_rate('annotator_disagreement', number_bins=None, filtering_value=None)\n",
    "results_test_annotator_FNR = test_metric_inclusion.false_negative_rate('annotator_disagreement', number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test_annotation_acc = test_metric_inclusion.average_accuracy('annotation_popularity', number_bins=None, filtering_value=None)\n",
    "results_test_annotation_TPR = test_metric_inclusion.true_positive_rate('annotation_popularity', number_bins=None, filtering_value=None)\n",
    "results_test_annotation_TNR = test_metric_inclusion.true_negative_rate('annotation_popularity', number_bins=None, filtering_value=None)\n",
    "results_test_annotation_FPR = test_metric_inclusion.false_positive_rate('annotation_popularity', number_bins=None, filtering_value=None)\n",
    "results_test_annotation_FNR = test_metric_inclusion.false_negative_rate('annotation_popularity', number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test_demography_acc = test_metric_inclusion.average_accuracy('demography', number_bins=None, filtering_value=None)\n",
    "results_test_demography_TPR = test_metric_inclusion.true_positive_rate('demography', number_bins=None, filtering_value=None)\n",
    "results_test_demography_TNR = test_metric_inclusion.true_negative_rate('demography', number_bins=None, filtering_value=None)\n",
    "results_test_demography_FPR = test_metric_inclusion.false_positive_rate('demography', number_bins=None, filtering_value=None)\n",
    "results_test_demography_FNR = test_metric_inclusion.false_negative_rate('demography', number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test_sample_acc = test_metric_inclusion.average_accuracy('sample_ambiguity', number_bins=None, filtering_value=None)\n",
    "results_test_sample_TNR = test_metric_inclusion.true_negative_rate('sample_ambiguity', number_bins=None, filtering_value=None)\n",
    "results_test_sample_TPR = test_metric_inclusion.true_positive_rate('sample_ambiguity', number_bins=None, filtering_value=None)\n",
    "results_test_sample_FPR = test_metric_inclusion.false_positive_rate('sample_ambiguity', number_bins=None, filtering_value=None)\n",
    "results_test_sample_FNR = test_metric_inclusion.false_negative_rate('sample_ambiguity', number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results computed on accuracy:\")\n",
    "print(\"Results: fairness on annotator\")\n",
    "print(\"Classifier with majority vote: \", results_test_annotator_acc)\n",
    "print(\"Results: fairness on samples\")\n",
    "print(\"Classifier with majority vote: \", results_test_sample_acc)\n",
    "print(\"Results: fairness on annotations\")\n",
    "print(\"Classifier with majority vote: \", results_test_annotation_acc)\n",
    "print(\"Results: fairness on demographic categories\")\n",
    "print(\"Classifier with majority vote: \", results_test_demography_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to transform the age, gender, education ordinal categories into human-readable categories (for later visualization).\n",
    "def translate_demography_index(data):\n",
    "    for index, row in data.iterrows():\n",
    "        print(index)\n",
    "        print(str(int(index)))\n",
    "        demog_translation = ''\n",
    "         \n",
    "        # Find the fist element (gender)\n",
    "        if len(str(int(index))) < 3:\n",
    "            # This means the gender is 0.\n",
    "            demog_translation += 'female'\n",
    "        else:\n",
    "            if str(int(index))[0] == '1':\n",
    "                demog_translation += 'male'\n",
    "            elif str(int(index))[0] == '2':\n",
    "                demog_translation += 'other'\n",
    "            elif str(int(index))[0] == '3':\n",
    "                demog_translation += 'nan'\n",
    "        # Find the second element:\n",
    "        demog_translation += ' '\n",
    "        if len(str(int(index))) < 2:\n",
    "            # This means the gender and age is 0.\n",
    "            demog_translation += 'Under 18'\n",
    "        else:\n",
    "            if str(int(index))[1] == '1':\n",
    "                demog_translation += '18-30'\n",
    "            elif str(int(index))[1] == '2':\n",
    "                demog_translation += '30-45'\n",
    "            elif str(int(index))[1] == '3':\n",
    "                demog_translation += '45-60'\n",
    "            elif str(int(index))[1] == '4':\n",
    "                demog_translation += 'Over 60'\n",
    "            elif str(int(index))[1] == '5':\n",
    "                demog_translation += 'nan'\n",
    "    \n",
    "        # Find the third element:\n",
    "        demog_translation += ' '\n",
    "        index_element = len(str(int(index))) - 1\n",
    "        \n",
    "        if str(int(index))[index_element] == '0':\n",
    "            demog_translation += 'none'\n",
    "        elif str(int(index))[index_element] == '1':\n",
    "            demog_translation += 'hs'\n",
    "        elif str(int(index))[index_element] == '2':\n",
    "            demog_translation += 'some'\n",
    "        elif str(int(index))[index_element] == '3':\n",
    "            demog_translation += 'bachelors'\n",
    "        elif str(int(index))[index_element] == '4':\n",
    "            demog_translation += 'masters'\n",
    "        elif str(int(index))[index_element] == '5':\n",
    "            demog_translation += 'professional'\n",
    "        elif str(int(index))[index_element] == '6':\n",
    "            demog_translation += 'doctorate'\n",
    "        elif str(int(index))[index_element] == '7':\n",
    "            demog_translation += 'nan'\n",
    "        data.rename(index={index:demog_translation}, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_demography_index(results_test_demography_acc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of unfairness visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_plot_color(value):\n",
    "    if value > 0.5:\n",
    "        return 'b'\n",
    "    else:\n",
    "        return 'w'\n",
    "\n",
    "def plot_fairness(data, metric, bin_name='', title_name=''):\n",
    "    yticks = list(data.index.values)\n",
    "\n",
    "    # Plot the bins\n",
    "    data = data.as_matrix(columns=[metric])\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(data)\n",
    "    cbar = fig.colorbar(heatmap, ax=ax)\n",
    "    cbar.set_label(metric, rotation=90)\n",
    "\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_yticklabels(yticks,rotation=0)\n",
    "    ax.set_ylabel(bin_name)\n",
    "    ax.tick_params(left='off', bottom='off',labelbottom='off', color='grey',labelsize='small')\n",
    "\n",
    "    # Add the exact evaluation measure per bin\n",
    "    for i in range(data.shape[0]):\n",
    "        text = ax.text(0.5, i+0.5, np.round(data[i][0], 2), ha=\"center\", va=\"center\", color=annotation_plot_color(data[i][0]))\n",
    "    if title_name != '':\n",
    "        ax.set_title(title_name)\n",
    "    fig.set_figwidth(4)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example visualisation of the results: comparison of the performance of the two classifiers on the different fairness-bins.\n",
    "plot_fairness(results_test_annotator_acc[2], 'accuracy', 'annotator disagreement rate', 'Performance of the classifier trained on the MV \\n based on annotator bins.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with the classifier trained on the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del test_metric_inclusion\n",
    "\n",
    "results_test_annotator_2_acc = test_metric_inclusion_2.average_accuracy('annotator_disagreement', number_bins=None, filtering_value=None)\n",
    "results_test_annotation_2_acc = test_metric_inclusion_2.average_accuracy('annotation_popularity', number_bins=None, filtering_value=None)\n",
    "results_test_annotation_2_TPR = test_metric_inclusion_2.true_positive_rate('annotation_popularity', number_bins=None, filtering_value=None)\n",
    "results_test_annotation_2_TNR = test_metric_inclusion_2.true_negative_rate('annotation_popularity', number_bins=None, filtering_value=None)\n",
    "results_test_annotation_2_FPR = test_metric_inclusion_2.false_positive_rate('annotation_popularity', number_bins=None, filtering_value=None)\n",
    "results_test_annotation_2_FNR = test_metric_inclusion_2.false_negative_rate('annotation_popularity', number_bins=None, filtering_value=None)\n",
    "results_test_demography_2_acc = test_metric_inclusion_2.average_accuracy('demography', number_bins=None, filtering_value=None)\n",
    "results_test_demography_2_TPR = test_metric_inclusion_2.true_positive_rate('demography', number_bins=None, filtering_value=None)\n",
    "results_test_sample_2_acc = test_metric_inclusion_2.average_accuracy('sample_ambiguity', number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results computed on accuracy:\")\n",
    "print(\"Results: fairness on annotator\")\n",
    "print(\"Classifier with majority vote: \", results_test_annotator_2_acc)\n",
    "print(\"Results: fairness on samples\")\n",
    "print(\"Classifier with majority vote: \", results_test_sample_2_acc)\n",
    "print(\"Results: fairness on annotations\")\n",
    "print(\"Classifier with majority vote: \", results_test_annotation_2_acc)\n",
    "print(\"Results: fairness on demographic categories\")\n",
    "print(\"Classifier with majority vote: \", results_test_demography_2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness(results_test_annotator_2_acc[2], 'accuracy', 'annotator disagreement rate', 'Performance of the classifier trained on the annotations \\n based on annotator bins.')\n",
    "# We observe that although the second classifier performs less well for the annotators who often agree with the majority (bin [0.001; 0.2]), it performs better for the annotators who disagree a lot with the majority vote (bin [0.8; 1.0]), what makes it globally fairer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the 2 classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compare results computed on accuracy:\")\n",
    "print(\"Results: fairness on annotator\")\n",
    "print(\"Classifier with majority vote: \", results_test_annotator_acc, \" Classifier with annotations: \", results_test_annotator_2_acc)\n",
    "print(\"Results: fairness on samples\")\n",
    "print(\"Classifier with majority vote: \", results_test_sample_acc, \" Classifier with annotations: \", results_test_sample_2_acc)\n",
    "print(\"Results: fairness on annotations\")\n",
    "print(\"Classifier with majority vote: \", results_test_annotation_acc, \" Classifier with annotations: \", results_test_annotation_2_acc)\n",
    "print(\"Results: fairness on demographic categories\")\n",
    "print(\"Classifier with majority vote: \", results_test_demography_acc, \" Classifier with annotations: \", results_test_demography_2_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
