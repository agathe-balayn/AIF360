{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring and characterizing fairness as a notion of inclusiveness.\n",
    "\n",
    "Certain Machine Learning models are made to perform classification tasks of samples over labels which are subjective, what means that several users of the models might judge the label of the sample differently depending on their personal experience.\n",
    "\n",
    "The predictions of the models might contain biases towards certain types of judgements which are more common than others and consequently easier to learn, and ignore other judgements. These biases might already be contained in the training dataset or generated by the classification model. \n",
    "\n",
    "However for the predictions to be fair towards each user of the model, they should be inclusive of all the different judgements, and possibly should be tuned to each of the users.\n",
    "\n",
    "In this tutorial we teach:\n",
    "- how to use metrics to measure how fair according to this notion of inclusiveness the models are,\n",
    "- and how to use various characterizations of the predictions to understand where the unfairness might come from.\n",
    "\n",
    "The tutorial is based on the example use-case of a Machine Learning model to classify the toxicity of a sentence (see image below).\n",
    "We train a classifier (Logistic Regression) using the toxicity dataset (sentences and toxicity labels) to predict sentence toxicity, and evaluate how fair the outputs of the process are based on the ground truth annotations provided by multiple judges (crowdsourcing annotators).\n",
    "\n",
    "![title](images/overview_tutorial_fairness_inclusiveness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")  \n",
    "\n",
    "import os\n",
    "\n",
    "from aif360.datasets import ToxicityDataset\n",
    "from aif360.metrics import InclusivenessLabelDatasetMetric\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as sk_met\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example to load the full toxicity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "import copy\n",
    "### The toxicity dataset (toxicity_annotations.tsv, toxicity_annotated_comments.tsv, toxicity_worker_demographics.tsv) should be downloaded from https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973\n",
    "### and placed in the folder \"data/raw/toxicity\".\n",
    "# Example on how to load the full dataset.\n",
    "#tox_dataset = ToxicityDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def clean_data(annotations, worker_demo, comments):\n",
    "        \n",
    "        # Preprocess workers.\n",
    "        worker_demo = worker_demo.replace(np.NaN, 'nan')\n",
    " \n",
    "        #### Add all the information to the annotations.\n",
    "        # Add the worker demographics.\n",
    "        annotations = annotations.reset_index().merge(worker_demo, on='worker_id', how='left').set_index(annotations.index.names)\n",
    "        # Remove the unknown demographics and the demographics with a NaN. And put them in a general test set.\n",
    "        annotations = annotations.replace(np.NaN, 'nan')\n",
    "        annotations.loc[((annotations['english_first_language'].str.contains('nan')) |(annotations['gender'].str.contains('nan')) | (annotations['age_group'].str.contains('nan')) | (annotations['education'].str.contains('nan')) ),'general_split'] = 1 #'test'\n",
    "        annotations = annotations.reset_index()\n",
    "        annotations['english_first_language'] = annotations['english_first_language'].replace('nan', 2)\n",
    "        annotations['pop_label'] = annotations[['gender', 'age_group', 'education']].apply(lambda x: ' '.join([str(x['gender']),str(x['age_group']), str(x['education'])]), axis=1)    \n",
    "          \n",
    "        # Add the comments in order to train / test the ML models.\n",
    "        annotations = annotations.reset_index().merge(comments[['comment']].reset_index(), on='rev_id', how='left').set_index('index')\n",
    "        return annotations\n",
    "    \n",
    "def normalize_text(text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        stopword_set = set(stopwords.words('english'))\n",
    "        stemmer = PorterStemmer()\n",
    "        # Convert text to lower-case and strip punctuation/symbols from words.\n",
    "        norm_text = text.lower()\n",
    "        # Replace breaks with spaces.\n",
    "        norm_text = norm_text.replace('<br />', ' ')\n",
    "        # Pad punctuation with spaces on both sides.\n",
    "        for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "            norm_text = norm_text.replace(char, ' ' + char + ' ') \n",
    "        # Tokenize.\n",
    "        norm_text = tokenizer.tokenize(norm_text)\n",
    "        # Remove stop words.\n",
    "        norm_text = [w for w in norm_text if not w in stopword_set]\n",
    "        norm_text = \" \".join(norm_text)\n",
    "        return norm_text\n",
    "\n",
    "    \n",
    "def clean_comments(comments):\n",
    "        comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "        comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "        comments['comment'] = comments['comment'].apply(lambda x: normalize_text(x))\n",
    "        return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the dataset.\n",
      "C:\\Users\\AgatheBalayn\\Documents\\thesis_related\\AIF360\\examples\\..\\aif360\\data\\raw\\toxicity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "### Read the documents.    \n",
    "try:\n",
    "    print(\"Load the dataset.\")\n",
    "    filepath = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), '..', 'aif360', 'data', 'raw', 'toxicity')\n",
    "    print(filepath)\n",
    "    comments = pd.read_csv(filepath + '/toxicity_annotated_comments.tsv', sep = '\\t', dtype={'rev_id':int, 'comment':str}, index_col = 0)\n",
    "    annotations = pd.read_csv(filepath + '/toxicity_annotations.tsv',  sep = '\\t', index_col=0)\n",
    "    worker_demo = pd.read_csv(filepath + '/toxicity_worker_demographics.tsv', sep='\\t')\n",
    "except IOError as err:\n",
    "    print(\"IOError: {}\".format(err))\n",
    "    print(\"To use this class, please download the following files from https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973:\")\n",
    "    print(\"\\n\\ttoxicity_annotated_comments.tsv\")\n",
    "    print(\"\\ttoxicity_annotations.tsv\")\n",
    "    print(\"\\ttoxicity_worker_demographics.tsv\")\n",
    "    print(\"\\nand place them, as-is, in the folder:\")\n",
    "    print(\"\\n\\t{}\\n\".format(os.path.abspath(os.path.join(os.path.abspath(__file__), '..', '..', 'data', 'raw', 'toxicity'))))\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "    \n",
    "### For now we do not use the whole dataset to be faster.\n",
    "n_lim = 50000\n",
    "comments = clean_comments(comments)\n",
    "annotations = annotations.head(n_lim)\n",
    "# Merge the different datasets.\n",
    "annotations = clean_data(annotations, worker_demo, comments)\n",
    "# Compute the ground truth (majority vote) label.\n",
    "annotations['MV'] = annotations.groupby(['rev_id'])['toxicity'].transform(lambda x : (x.mean() >= 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the unique comments for training and testing of the ML models.\n",
    "def prepare_aggregated_data(comment_):\n",
    "    # Get the unique comments.\n",
    "    comments = comment_.drop_duplicates('rev_id')\n",
    "    # Cleaning.\n",
    "    comments = comments[['comment', 'MV']]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the training and test sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Training and test sets with annotations as ground truth:\n",
    "annotations_train, annotations_test = train_test_split(annotations, test_size=0.3)\n",
    "# Training and test sets with majority-vote as ground truth:\n",
    "comments_train = prepare_aggregated_data(annotations_train)\n",
    "comments_test = prepare_aggregated_data(annotations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load the model.\n",
    "class DataFrameColumnExtracter_doc(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.9597758655193116\n",
      "Test accuracy:  0.9596607364501448\n",
      "Training confusion matrix: [[0.95954357 0.03389831]\n",
      " [0.04045643 0.96610169]]\n",
      "Test confusion matrix: [[0.95944206 0.03448276]\n",
      " [0.04055794 0.96551724]]\n"
     ]
    }
   ],
   "source": [
    "### The ML model is then trained on the majority vote labels. \n",
    "\n",
    "## 1) Perform grid search over the parameters of the model.\n",
    "\n",
    "# Load the model, here Logistic Regression model.\n",
    "clf_LR = Pipeline([# Sentences.\n",
    "                  ('sentences_features', Pipeline([\n",
    "                      ('sentence_extractor', DataFrameColumnExtracter_doc('comment')),#.values.astype('U'),\n",
    "                    ('vect', CountVectorizer(max_features = 1500, ngram_range = (1,5), analyzer = 'char')),\n",
    "                     ('tf', TfidfTransformer(norm = 'l2'))\n",
    "                  ])),\n",
    "            # Classifier.\n",
    "            ('clf', LogisticRegression())#C=LR_C, tol=LR_C_tol))\n",
    "        ])\n",
    "\n",
    "# Parameters of the grid search:\n",
    "tuned_parameters = {'clf__C': [1e-4, 1e-2, 1, 10], 'clf__tol': [1, 1e-2, 1e-4]} \n",
    "\n",
    "# Initialize the grid search.\n",
    "clf = GridSearchCV(clf_LR, tuned_parameters, cv=5, verbose=0)\n",
    "\n",
    "# Functions to save and load the results of the grid search.\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "# Use a small number of data to train the model faster (this is used for quick testing).\n",
    "nb_data = 5000\n",
    "\n",
    "# Train the grid search.\n",
    "# To comment if already run ones and the paramters were saved.\n",
    "# =============\n",
    "#best_model = clf.fit(dataset_train_comments[0:nb_data], dataset_train_comments['MV'][0:nb_data])\n",
    "#best_parameters = best_model.best_params_  \n",
    "#print(best_parameters)  \n",
    "#best_result = best_model.best_score_  \n",
    "#print(best_result)  \n",
    "#save_obj(best_parameters, 'best_param_LR_aggregated')\n",
    "# =============\n",
    "\n",
    "\n",
    "## 2) Train the final model.\n",
    "best_parameters = load_obj('best_param_LR_aggregated')\n",
    "clf_LR.set_params(**best_parameters)\n",
    "clf_LR.fit(comments_train[0:nb_data], comments_train['MV'][0:nb_data])\n",
    "\n",
    "\n",
    "## 3) Evaluate general performance.\n",
    "train_pred = clf_LR.predict(comments_train[0:nb_data])\n",
    "test_pred = clf_LR.predict(comments_test[0:nb_data])\n",
    "print(\"Training accuracy: \", sk_met.accuracy_score(comments_train['MV'][0:nb_data], train_pred))\n",
    "print(\"Test accuracy: \", sk_met.accuracy_score(comments_test['MV'][0:nb_data], test_pred))\n",
    "\n",
    "C_train = sk_met.confusion_matrix(comments_train['MV'][0:nb_data], train_pred)\n",
    "C_train = C_train / C_train.astype(np.float).sum(axis=0)\n",
    "C_test = sk_met.confusion_matrix(comments_test['MV'][0:nb_data], test_pred)\n",
    "C_test = C_test / C_test.astype(np.float).sum(axis=0)\n",
    "print(\"Training confusion matrix:\", C_train)\n",
    "print(\"Test confusion matrix:\", C_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.92\n",
      "Test accuracy:  0.9058\n",
      "Training confusion matrix: [[0.92016632 0.08421053]\n",
      " [0.07983368 0.91578947]]\n",
      "Test confusion matrix: [[0.91558038 0.37724551]\n",
      " [0.08441962 0.62275449]]\n"
     ]
    }
   ],
   "source": [
    "### Train a second ML model on the annotation (not majority-vote) labels in order to compare the fairness measures later. \n",
    "\n",
    "## 1) Perform grid search over the parameters of the model.\n",
    "\n",
    "# Load the model, here Logistic Regression model.\n",
    "clf_LR_annotations = Pipeline([# Sentences.\n",
    "                  ('sentences_features', Pipeline([\n",
    "                      ('sentence_extractor', DataFrameColumnExtracter_doc('comment')),#.values.astype('U'),\n",
    "                    ('vect', CountVectorizer(max_features = 1500, ngram_range = (1,5), analyzer = 'char')),\n",
    "                     ('tf', TfidfTransformer(norm = 'l2'))\n",
    "                  ])),\n",
    "            # Classifier.\n",
    "            ('clf', LogisticRegression())#C=LR_C, tol=LR_C_tol))\n",
    "        ])\n",
    "\n",
    "# Parameters of the grid search:\n",
    "tuned_parameters = {'clf__C': [1e-4, 1e-2, 1, 10], 'clf__tol': [1, 1e-2, 1e-4]} \n",
    "\n",
    "# Initialize the grid search.\n",
    "clf_annotations = GridSearchCV(clf_LR_annotations, tuned_parameters, cv=5, verbose=0)\n",
    "    \n",
    "# Use a small number of data to train the model faster.\n",
    "nb_data = 5000\n",
    "\n",
    "# Train the grid search.\n",
    "# To comment if already run ones and the paramters were saved.\n",
    "# =============\n",
    "#best_model = clf_annotations.fit(annotations_train[0:nb_data], annotations_train['toxicity'][0:nb_data])\n",
    "#best_parameters = best_model.best_params_  \n",
    "#print(best_parameters)  \n",
    "#best_result = best_model.best_score_  \n",
    "#print(best_result)  \n",
    "#save_obj(best_parameters, 'best_param_LR_annotations')\n",
    "# =============\n",
    "\n",
    "\n",
    "## 2) Train the final model.\n",
    "best_parameters = load_obj('best_param_LR_annotations')\n",
    "clf_LR_annotations.set_params(**best_parameters)\n",
    "clf_LR_annotations.fit(annotations_train[0:nb_data], annotations_train['toxicity'][0:nb_data])\n",
    "\n",
    "\n",
    "## 3) Evaluate general performance.\n",
    "train_pred = clf_LR_annotations.predict(annotations_train[0:nb_data])\n",
    "test_pred = clf_LR_annotations.predict(annotations_test[0:nb_data])\n",
    "print(\"Training accuracy: \", sk_met.accuracy_score(annotations_train['toxicity'][0:nb_data], train_pred))\n",
    "print(\"Test accuracy: \", sk_met.accuracy_score(annotations_test['toxicity'][0:nb_data], test_pred))\n",
    "\n",
    "C_train = sk_met.confusion_matrix(annotations_train['toxicity'][0:nb_data], train_pred)\n",
    "C_train = C_train / C_train.astype(np.float).sum(axis=0)\n",
    "C_test = sk_met.confusion_matrix(annotations_test['toxicity'][0:nb_data], test_pred)\n",
    "C_test = C_test / C_test.astype(np.float).sum(axis=0)\n",
    "print(\"Training confusion matrix:\", C_train)\n",
    "print(\"Test confusion matrix:\", C_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the fairness performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import SubjectivityDataset\n",
    "\n",
    "def default_preprocessing(df):\n",
    "    return df\n",
    "\n",
    "# Wrapper to load the datasets to compute the fairness on.\n",
    "def subjectivity_dataset_wrapper(annotations, label_name,\n",
    "                 protected_attribute_names=['gender', 'english_first_language', 'age_group', 'education', 'rev_id', 'worker_id', 'pop_label'], privileged_classes=None,\n",
    "                 instance_weights_name=None, categorical_features=[],\n",
    "                 features_to_keep=[], features_to_drop=[], na_values=[],\n",
    "                 custom_preprocessing=default_preprocessing, \n",
    "                metadata={'label_maps': [{1.0: 'Toxic', 0.0: 'Non-toxic'}],},\n",
    "                mapping_categorical_protected=(('gender',('female','male', 'other', 'nan')), ('age_group',('Under 18', '18-30', '30-45', '45-60', 'Over 60', 'nan')), ('education',('none', 'hs', 'some', 'bachelors', 'masters', 'professional', 'doctorate', 'nan')))):\n",
    "    \n",
    "    if 'comment' in annotations.columns.tolist():\n",
    "        annotations = annotations.drop('comment', axis=1)\n",
    "    if 'general_split' in annotations.columns.tolist():\n",
    "        annotations = annotations.drop('general_split', axis=1)\n",
    "          \n",
    "    # Create the ground truth data\n",
    "    if 'toxicity' in annotations.columns.tolist():\n",
    "        annotations['GT'] = annotations['toxicity'].copy()\n",
    "    if label_name != 'toxicity':\n",
    "        # Delete the 'toxicity' column.\n",
    "        if 'toxicity' in annotations.columns.tolist():\n",
    "            annotations = annotations.drop('toxicity', axis=1)\n",
    "        annotations.rename(columns={label_name:'toxicity'}, inplace=True)\n",
    "        label_name = 'toxicity'\n",
    "    else:\n",
    "        if 'pred_1' in annotations.columns.tolist():\n",
    "            annotations = annotations.drop('pred_1', axis=1)\n",
    "    annotations = annotations.astype({\"toxicity\": float})\n",
    "\n",
    "    # Make the categorical data numbers.\n",
    "    if mapping_categorical_protected != ():\n",
    "        for tuple_type in mapping_categorical_protected:\n",
    "            for tuple_details in tuple_type:\n",
    "                if tuple_type.index(tuple_details) == 0:\n",
    "                    key = tuple_details\n",
    "                else:\n",
    "                    for tuple_categories in tuple_details:\n",
    "                        annotations[key] = annotations[key].replace(tuple_categories, tuple_details.index(tuple_categories))\n",
    "\n",
    "    annotations['pop_label'] = annotations[['gender', 'age_group', 'education']].apply(lambda x: int(''.join([str(x['gender']),str(x['age_group']), str(x['education'])])), axis=1)    \n",
    "    annotations = annotations[['rev_id', 'worker_id', 'toxicity', 'toxicity_score', 'gender', 'english_first_language', 'age_group', 'education', 'pop_label', 'GT', 'MV']]\n",
    "    \n",
    "    dataset = SubjectivityDataset(annotations, label_name, 'GT',\n",
    "                 protected_attribute_names=protected_attribute_names, privileged_classes=privileged_classes,\n",
    "                 instance_weights_name=instance_weights_name, categorical_features=categorical_features,\n",
    "                 features_to_keep=features_to_keep, features_to_drop=features_to_drop, na_values=na_values,\n",
    "                 custom_preprocessing=custom_preprocessing, metadata=metadata)\n",
    "    return dataset, annotations\n",
    "\n",
    "\n",
    "# The dataset might be too large for the classifier to process all the data. In this case, it is splitted to get the predictions on all the data.\n",
    "def compute_pred_dataset(classifier, dataset_orig_train, prediction_col, nb_data):\n",
    "    for i in range(int(len(dataset_orig_train) / nb_data)):\n",
    "        print(i)\n",
    "        low_interval = i*nb_data\n",
    "        high_interval = (i+1)*nb_data\n",
    "        dataset_orig_train[prediction_col].iloc[low_interval:high_interval] = classifier.predict(dataset_orig_train.iloc[low_interval:high_interval])\n",
    "    if high_interval < len(dataset_orig_train):\n",
    "        dataset_orig_train[prediction_col].iloc[high_interval:] = classifier.predict(dataset_orig_train.iloc[high_interval:])\n",
    "    return dataset_orig_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "### Create SubjectivityDataset datasets out of the training and test results and ground truth on the classifier trained on the MV.\n",
    "\n",
    "prediction_col = \"pred_1\"\n",
    "annotations_train[prediction_col] = -1\n",
    "annotations_test[prediction_col] = -1\n",
    "nb_data = 5000\n",
    "\n",
    "subjectivity_dataset_test_GT, annotations_subjectivity_dataset_test_GT = subjectivity_dataset_wrapper(annotations_test, 'toxicity')\n",
    "annotations_test_pred = compute_pred_dataset(clf_LR, annotations_test, prediction_col, nb_data)\n",
    "subjectivity_dataset_test_outputs, annotations_subjectivity_dataset_test_outputs = subjectivity_dataset_wrapper(annotations_test_pred, prediction_col)\n",
    "\n",
    "# Instantiate the fairness metric class.\n",
    "test_metric_inclusion = InclusivenessLabelDatasetMetric(subjectivity_dataset_test_GT, subjectivity_dataset_test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "### Create SubjectivityDataset datasets out of the training and test results and ground truth on the classifier trained on the annotations.\n",
    "annotations_test_pred_2 = compute_pred_dataset(clf_LR_annotations, annotations_test, prediction_col, nb_data)\n",
    "subjectivity_dataset_test_outputs_2, annotations_subjectivity_dataset_test_outputs_2 = subjectivity_dataset_wrapper(annotations_test_pred, prediction_col)\n",
    "\n",
    "# Instantiate the fairness metric class.\n",
    "test_metric_inclusion_2 = InclusivenessLabelDatasetMetric(subjectivity_dataset_test_GT, subjectivity_dataset_test_outputs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aif360\\metrics\\classification_metric.py:270: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "..\\aif360\\metrics\\classification_metric.py:271: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:151: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  matrix_data = data.as_matrix([metric_to_compute])\n",
      "..\\aif360\\metrics\\classification_metric.py:270: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "..\\aif360\\metrics\\classification_metric.py:271: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:151: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  matrix_data = data.as_matrix([metric_to_compute])\n"
     ]
    }
   ],
   "source": [
    "results_test_annotator = test_metric_inclusion.compute_bin_metrics('annotator_disagreement', ('accuracy',), number_bins=None, filtering_value=None)\n",
    "results_test_annotator_2 = test_metric_inclusion_2.compute_bin_metrics('annotator_disagreement', ('accuracy',), number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:151: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  matrix_data = data.as_matrix([metric_to_compute])\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:151: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  matrix_data = data.as_matrix([metric_to_compute])\n"
     ]
    }
   ],
   "source": [
    "results_test_annotation = test_metric_inclusion.compute_bin_metrics('annotation_popularity', ('accuracy','TPR', 'TNR', 'FPR', 'FNR'), number_bins=None, filtering_value=None)\n",
    "results_test_annotation_2 = test_metric_inclusion_2.compute_bin_metrics('annotation_popularity', ('accuracy','TPR', 'TNR', 'FPR', 'FNR'), number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aif360\\metrics\\classification_metric.py:270: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "..\\aif360\\metrics\\classification_metric.py:271: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n"
     ]
    }
   ],
   "source": [
    "results_test_demography = test_metric_inclusion.compute_bin_metrics('demography', ('accuracy', 'TPR'), number_bins=None, filtering_value=None)\n",
    "results_test_demography_2 = test_metric_inclusion_2.compute_bin_metrics('demography', ('accuracy', 'TPR'), number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aif360\\metrics\\classification_metric.py:270: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "..\\aif360\\metrics\\classification_metric.py:271: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:151: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  matrix_data = data.as_matrix([metric_to_compute])\n",
      "..\\aif360\\metrics\\classification_metric.py:270: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "..\\aif360\\metrics\\classification_metric.py:271: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:151: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  matrix_data = data.as_matrix([metric_to_compute])\n"
     ]
    }
   ],
   "source": [
    "results_test_sample = test_metric_inclusion.compute_bin_metrics('sample_ambiguity', ('accuracy',), number_bins=None, filtering_value=None)\n",
    "results_test_sample_2 = test_metric_inclusion_2.compute_bin_metrics('sample_ambiguity', ('accuracy',), number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: fairness on annotator\n",
      "Classifier with majority vote:  [(0.7889168587645954, 0.6254107586790191,                accuracy\n",
      "bin_col                \n",
      "(-0.001, 0.2]  0.945271\n",
      "(0.2, 0.4]     0.761039\n",
      "(0.4, 0.6]     0.621385\n",
      "(0.6, 0.8]     0.391026\n",
      "(0.8, 1.0]     0.408333)]  Classifier with annotations:  [(0.8170791879833249, 0.644303240019641,                accuracy\n",
      "bin_col                \n",
      "(-0.001, 0.2]  0.930994\n",
      "(0.2, 0.4]     0.755707\n",
      "(0.4, 0.6]     0.625841\n",
      "(0.6, 0.8]     0.442308\n",
      "(0.8, 1.0]     0.466667)]\n",
      "Results: fairness on samples\n",
      "Classifier with majority vote:  [(0.8361131831323065, 0.723516938309469,             accuracy\n",
      "bin_col             \n",
      "(0.5, 0.6]  0.510178\n",
      "(0.6, 0.7]  0.596639\n",
      "(0.7, 0.8]  0.714203\n",
      "(0.8, 0.9]  0.822421\n",
      "(0.9, 1.0]  0.974144)]  Classifier with annotations:  [(0.8421840080585414, 0.7105789078700903,             accuracy\n",
      "bin_col             \n",
      "(0.5, 0.6]  0.505886\n",
      "(0.6, 0.7]  0.593838\n",
      "(0.7, 0.8]  0.701896\n",
      "(0.8, 0.9]  0.790675\n",
      "(0.9, 1.0]  0.960601)]\n",
      "Results: fairness on annotations\n",
      "Classifier with majority vote:  [(0.6576026816905403, 0.5458066293311682,                accuracy\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.088235\n",
      "(0.3, 0.475]   0.245232\n",
      "(0.475, 0.65]  0.558029\n",
      "(0.65, 0.825]  0.855083\n",
      "(0.825, 1.0]   0.982455), (0.7791537224500777, 0.2052368233246696,                     TPR\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.007576\n",
      "(0.3, 0.475]   0.011719\n",
      "(0.475, 0.65]  0.135135\n",
      "(0.65, 0.825]  0.268482\n",
      "(0.825, 1.0]   0.603272), (0.847050672194235, 0.8599180901882779,                     TNR\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.595238\n",
      "(0.3, 0.475]   0.783784\n",
      "(0.475, 0.65]  0.933934\n",
      "(0.65, 0.825]  0.988496\n",
      "(0.825, 1.0]   0.998139), (0.847050672194235, 0.14008190981172206,                     FPR\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.404762\n",
      "(0.3, 0.475]   0.216216\n",
      "(0.475, 0.65]  0.066066\n",
      "(0.65, 0.825]  0.011504\n",
      "(0.825, 1.0]   0.001861), (0.7791537224500777, 0.7947631766753304,                     FNR\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.992424\n",
      "(0.3, 0.475]   0.988281\n",
      "(0.475, 0.65]  0.864865\n",
      "(0.65, 0.825]  0.731518\n",
      "(0.825, 1.0]   0.396728)]  Classifier with annotations:  [(0.6786315647321434, 0.5472229870472971,                accuracy\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.117647\n",
      "(0.3, 0.475]   0.280654\n",
      "(0.475, 0.65]  0.534181\n",
      "(0.65, 0.825]  0.834174\n",
      "(0.825, 1.0]   0.969458), (0.8449285614551731, 0.16833631450628536,                     TPR\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.026515\n",
      "(0.3, 0.475]   0.031250\n",
      "(0.475, 0.65]  0.101351\n",
      "(0.65, 0.825]  0.249027\n",
      "(0.825, 1.0]   0.433538), (0.8923204306241258, 0.8848266769717252,                     TNR\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.690476\n",
      "(0.3, 0.475]   0.855856\n",
      "(0.475, 0.65]  0.918919\n",
      "(0.65, 0.825]  0.967257\n",
      "(0.825, 1.0]   0.991626), (0.8923204306241257, 0.11517332302827472,                     FPR\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.309524\n",
      "(0.3, 0.475]   0.144144\n",
      "(0.475, 0.65]  0.081081\n",
      "(0.65, 0.825]  0.032743\n",
      "(0.825, 1.0]   0.008374), (0.8449285614551731, 0.8316636854937146,                     FNR\n",
      "bin_col                \n",
      "(0.124, 0.3]   0.973485\n",
      "(0.3, 0.475]   0.968750\n",
      "(0.475, 0.65]  0.898649\n",
      "(0.65, 0.825]  0.750973\n",
      "(0.825, 1.0]   0.566462)]\n",
      "Results: fairness on demographic categories\n",
      "Classifier with majority vote:  [(0.8713358755153158, 0.8907650494258962,            accuracy\n",
      "pop_label          \n",
      "111.0      0.922283\n",
      "123.0      0.909445\n",
      "35.0       0.840000\n",
      "23.0       0.907975\n",
      "11.0       0.916667\n",
      "121.0      0.927914\n",
      "21.0       0.923077\n",
      "113.0      0.932124\n",
      "357.0      0.924088\n",
      "14.0       0.906250\n",
      "31.0       0.901316\n",
      "115.0      0.929254\n",
      "153.0      0.966102\n",
      "114.0      0.947464\n",
      "124.0      0.915541\n",
      "125.0      0.894523\n",
      "13.0       0.879070\n",
      "131.0      0.907285\n",
      "134.0      0.858696\n",
      "112.0      0.915966\n",
      "151.0      0.985915\n",
      "135.0      0.886076\n",
      "101.0      0.940741\n",
      "53.0       0.896552\n",
      "15.0       0.936975\n",
      "32.0       1.000000\n",
      "1.0        0.775000\n",
      "34.0       0.861538\n",
      "25.0       0.912821\n",
      "24.0       0.868778\n",
      "...             ...\n",
      "22.0       0.969697\n",
      "51.0       0.933333\n",
      "126.0      0.931818\n",
      "33.0       0.942446\n",
      "122.0      0.918367\n",
      "42.0       1.000000\n",
      "54.0       0.852941\n",
      "26.0       0.888889\n",
      "44.0       0.909091\n",
      "143.0      0.750000\n",
      "16.0       0.937500\n",
      "145.0      1.000000\n",
      "116.0      0.962264\n",
      "132.0      0.846154\n",
      "43.0       1.000000\n",
      "3.0        1.000000\n",
      "104.0      0.800000\n",
      "2.0        0.800000\n",
      "45.0       0.761905\n",
      "46.0       1.000000\n",
      "102.0      0.960000\n",
      "136.0      0.833333\n",
      "155.0      0.900000\n",
      "41.0       0.875000\n",
      "201.0      0.800000\n",
      "152.0      0.833333\n",
      "52.0       1.000000\n",
      "110.0      0.750000\n",
      "56.0       1.000000\n",
      "36.0       0.000000\n",
      "\n",
      "[64 rows x 1 columns]), (0.7819998690618737, 0.23146788133188986,                 TPR\n",
      "pop_label          \n",
      "111.0      0.321918\n",
      "123.0      0.273504\n",
      "35.0       0.400000\n",
      "23.0       0.294872\n",
      "11.0       0.263158\n",
      "121.0      0.333333\n",
      "21.0       0.292683\n",
      "113.0      0.269461\n",
      "357.0      0.251185\n",
      "14.0       0.340909\n",
      "31.0       0.166667\n",
      "115.0      0.333333\n",
      "153.0      0.750000\n",
      "114.0      0.351351\n",
      "124.0      0.186441\n",
      "125.0      0.209677\n",
      "13.0       0.141593\n",
      "131.0      0.125000\n",
      "134.0      0.235294\n",
      "112.0      0.250000\n",
      "151.0      0.000000\n",
      "135.0      0.333333\n",
      "101.0      0.200000\n",
      "53.0       0.250000\n",
      "15.0       0.480000\n",
      "32.0            NaN\n",
      "1.0        0.100000\n",
      "34.0       0.181818\n",
      "25.0       0.157895\n",
      "24.0       0.194444\n",
      "...             ...\n",
      "22.0       0.000000\n",
      "51.0       0.000000\n",
      "126.0      0.400000\n",
      "33.0       0.384615\n",
      "122.0      0.250000\n",
      "42.0       1.000000\n",
      "54.0       0.000000\n",
      "26.0       0.250000\n",
      "44.0       0.500000\n",
      "143.0      0.333333\n",
      "16.0       0.000000\n",
      "145.0           NaN\n",
      "116.0      0.000000\n",
      "132.0      0.333333\n",
      "43.0            NaN\n",
      "3.0             NaN\n",
      "104.0      0.000000\n",
      "2.0        0.000000\n",
      "45.0       0.166667\n",
      "46.0       1.000000\n",
      "102.0      0.500000\n",
      "136.0      0.000000\n",
      "155.0      0.000000\n",
      "41.0       0.000000\n",
      "201.0      0.000000\n",
      "152.0      0.000000\n",
      "52.0            NaN\n",
      "110.0      0.000000\n",
      "56.0            NaN\n",
      "36.0       0.000000\n",
      "\n",
      "[64 rows x 1 columns])]  Classifier with annotations:  [(0.8690762798763716, 0.8742958715493641,            accuracy\n",
      "pop_label          \n",
      "111.0      0.914030\n",
      "123.0      0.897760\n",
      "35.0       0.800000\n",
      "23.0       0.892638\n",
      "11.0       0.912963\n",
      "121.0      0.904908\n",
      "21.0       0.894231\n",
      "113.0      0.926425\n",
      "357.0      0.913062\n",
      "14.0       0.887500\n",
      "31.0       0.875000\n",
      "115.0      0.898662\n",
      "153.0      0.915254\n",
      "114.0      0.943841\n",
      "124.0      0.910473\n",
      "125.0      0.890467\n",
      "13.0       0.868605\n",
      "131.0      0.907285\n",
      "134.0      0.858696\n",
      "112.0      0.890756\n",
      "151.0      0.985915\n",
      "135.0      0.848101\n",
      "101.0      0.925926\n",
      "53.0       0.862069\n",
      "15.0       0.924370\n",
      "32.0       1.000000\n",
      "1.0        0.725000\n",
      "34.0       0.830769\n",
      "25.0       0.907692\n",
      "24.0       0.859729\n",
      "...             ...\n",
      "22.0       0.969697\n",
      "51.0       0.933333\n",
      "126.0      0.954545\n",
      "33.0       0.935252\n",
      "122.0      0.897959\n",
      "42.0       1.000000\n",
      "54.0       0.852941\n",
      "26.0       0.888889\n",
      "44.0       0.818182\n",
      "143.0      0.750000\n",
      "16.0       1.000000\n",
      "145.0      1.000000\n",
      "116.0      0.962264\n",
      "132.0      0.846154\n",
      "43.0       1.000000\n",
      "3.0        1.000000\n",
      "104.0      0.800000\n",
      "2.0        0.900000\n",
      "45.0       0.666667\n",
      "46.0       0.800000\n",
      "102.0      0.840000\n",
      "136.0      0.833333\n",
      "155.0      0.850000\n",
      "41.0       0.875000\n",
      "201.0      0.800000\n",
      "152.0      0.833333\n",
      "52.0       1.000000\n",
      "110.0      0.750000\n",
      "56.0       1.000000\n",
      "36.0       0.000000\n",
      "\n",
      "[64 rows x 1 columns]), (0.7857834284368742, 0.1928942754645406,                 TPR\n",
      "pop_label          \n",
      "111.0      0.267123\n",
      "123.0      0.205128\n",
      "35.0       0.300000\n",
      "23.0       0.243590\n",
      "11.0       0.228070\n",
      "121.0      0.151515\n",
      "21.0       0.146341\n",
      "113.0      0.275449\n",
      "357.0      0.194313\n",
      "14.0       0.227273\n",
      "31.0       0.055556\n",
      "115.0      0.156863\n",
      "153.0      0.500000\n",
      "114.0      0.297297\n",
      "124.0      0.203390\n",
      "125.0      0.161290\n",
      "13.0       0.097345\n",
      "131.0      0.125000\n",
      "134.0      0.235294\n",
      "112.0      0.166667\n",
      "151.0      0.000000\n",
      "135.0      0.083333\n",
      "101.0      0.300000\n",
      "53.0       0.125000\n",
      "15.0       0.360000\n",
      "32.0            NaN\n",
      "1.0        0.100000\n",
      "34.0       0.000000\n",
      "25.0       0.157895\n",
      "24.0       0.194444\n",
      "...             ...\n",
      "22.0       0.000000\n",
      "51.0       0.000000\n",
      "126.0      0.600000\n",
      "33.0       0.384615\n",
      "122.0      0.250000\n",
      "42.0       1.000000\n",
      "54.0       0.200000\n",
      "26.0       0.250000\n",
      "44.0       0.500000\n",
      "143.0      0.333333\n",
      "16.0       1.000000\n",
      "145.0           NaN\n",
      "116.0      0.000000\n",
      "132.0      0.333333\n",
      "43.0            NaN\n",
      "3.0             NaN\n",
      "104.0      0.000000\n",
      "2.0        0.500000\n",
      "45.0       0.000000\n",
      "46.0       0.000000\n",
      "102.0      0.000000\n",
      "136.0      0.000000\n",
      "155.0      0.000000\n",
      "41.0       0.000000\n",
      "201.0      0.000000\n",
      "152.0      0.000000\n",
      "52.0            NaN\n",
      "110.0      0.000000\n",
      "56.0            NaN\n",
      "36.0       0.000000\n",
      "\n",
      "[64 rows x 1 columns])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Results: fairness on annotator\")\n",
    "print(\"Classifier with majority vote: \", results_test_annotator, \" Classifier with annotations: \", results_test_annotator_2)\n",
    "print(\"Results: fairness on samples\")\n",
    "print(\"Classifier with majority vote: \", results_test_sample, \" Classifier with annotations: \", results_test_sample_2)\n",
    "print(\"Results: fairness on annotations\")\n",
    "print(\"Classifier with majority vote: \", results_test_annotation, \" Classifier with annotations: \", results_test_annotation_2)\n",
    "print(\"Results: fairness on demographic categories\")\n",
    "print(\"Classifier with majority vote: \", results_test_demography, \" Classifier with annotations: \", results_test_demography_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_demography_index(data):\n",
    "    for index, row in data.iterrows():\n",
    "        print(index)\n",
    "        print(str(int(index)))\n",
    "        demog_translation = ''\n",
    "         \n",
    "        # Find the fist element (gender)\n",
    "        if len(str(int(index))) < 3:\n",
    "            # This means the gender is 0.\n",
    "            demog_translation += 'female'\n",
    "        else:\n",
    "            if str(int(index))[0] == '1':\n",
    "                demog_translation += 'male'\n",
    "            elif str(int(index))[0] == '2':\n",
    "                demog_translation += 'other'\n",
    "            elif str(int(index))[0] == '3':\n",
    "                demog_translation += 'nan'\n",
    "        # Find the second element:\n",
    "        demog_translation += ' '\n",
    "        if len(str(int(index))) < 2:\n",
    "            # This means the gender and age is 0.\n",
    "            demog_translation += 'Under 18'\n",
    "        else:\n",
    "            if str(int(index))[1] == '1':\n",
    "                demog_translation += '18-30'\n",
    "            elif str(int(index))[1] == '2':\n",
    "                demog_translation += '30-45'\n",
    "            elif str(int(index))[1] == '3':\n",
    "                demog_translation += '45-60'\n",
    "            elif str(int(index))[1] == '4':\n",
    "                demog_translation += 'Over 60'\n",
    "            elif str(int(index))[1] == '5':\n",
    "                demog_translation += 'nan'\n",
    "    \n",
    "        # Find the third element:\n",
    "        demog_translation += ' '\n",
    "        index_element = len(str(int(index))) - 1\n",
    "        \n",
    "        if str(int(index))[index_element] == '0':\n",
    "            demog_translation += 'none'\n",
    "        elif str(int(index))[index_element] == '1':\n",
    "            demog_translation += 'hs'\n",
    "        elif str(int(index))[index_element] == '2':\n",
    "            demog_translation += 'some'\n",
    "        elif str(int(index))[index_element] == '3':\n",
    "            demog_translation += 'bachelors'\n",
    "        elif str(int(index))[index_element] == '4':\n",
    "            demog_translation += 'masters'\n",
    "        elif str(int(index))[index_element] == '5':\n",
    "            demog_translation += 'professional'\n",
    "        elif str(int(index))[index_element] == '6':\n",
    "            demog_translation += 'doctorate'\n",
    "        elif str(int(index))[index_element] == '7':\n",
    "            demog_translation += 'nan'\n",
    "        data.rename(index={index:demog_translation}, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_demography_index(results_test_demography[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_plot_color(value):\n",
    "    if value > 0.5:\n",
    "        return 'b'\n",
    "    else:\n",
    "        return 'w'\n",
    "\n",
    "def plot_fairness(data, metric, bin_name='', title_name=''):\n",
    "    yticks = list(data.index.values)\n",
    "\n",
    "    # Plot the bins\n",
    "    data = data.as_matrix(columns=[metric])\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(data)\n",
    "    cbar = fig.colorbar(heatmap, ax=ax)\n",
    "    cbar.set_label(metric, rotation=90)\n",
    "\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_yticklabels(yticks,rotation=0)\n",
    "    ax.set_ylabel(bin_name)\n",
    "    ax.tick_params(left='off', bottom='off',labelbottom='off', color='grey',labelsize='small')\n",
    "\n",
    "    # Add the exact evaluation measure per bin\n",
    "    for i in range(data.shape[0]):\n",
    "        text = ax.text(0.5, i+0.5, np.round(data[i][0], 2), ha=\"center\", va=\"center\", color=annotation_plot_color(data[i][0]))\n",
    "    if title_name != '':\n",
    "        ax.set_title(title_name)\n",
    "    fig.set_figwidth(4)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example visualisation of the results: comparison of the performance of the two classifiers on the different fairness-bins.\n",
    "plot_fairness(results_test_annotator[0][2], 'accuracy', 'annotator disagreement rate', 'Performance of the classifier trained on the MV \\n based on annotator bins.')\n",
    "plot_fairness(results_test_annotator_2[0][2], 'accuracy', 'annotator disagreement rate', 'Performance of the classifier trained on the annotations \\n based on annotator bins.')\n",
    "# We observe that although the second classifier performs less well for the annotators who often agree with the majority (bin [0.001; 0.2]), it performs better for the annotators who disagree a lot with the majority vote (bin [0.8; 1.0]), what makes it globally fairer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
