{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring and characterizing fairness as a notion of inclusiveness.\n",
    "\n",
    "Certain Machine Learning models are made to perform classification tasks of samples over labels which are subjective, what means that several users of the models might judge the label of the sample differently depending on their personal experience.\n",
    "\n",
    "The predictions of the models might contain biases towards certain types of judgements which are more common than others and consequently easier to learn, and ignore other judgements. These biases might already be contained in the training dataset or generated by the classification model. \n",
    "\n",
    "However for the predictions to be fair towards each user of the model, they should be inclusive of all the different judgements, and possibly should be tuned to each of the users.\n",
    "\n",
    "In this tutorial we teach:\n",
    "- how to use metrics to measure how fair according to this notion of inclusiveness the models are,\n",
    "- and how to use various characterizations of the predictions to understand where the unfairness might come from.\n",
    "\n",
    "The tutorial is based on the example use-case of a Machine Learning model to classify the toxicity of a sentence (see image below).\n",
    "We train a classifier (Logistic Regression) using the toxicity dataset (sentences and toxicity labels) to predict sentence toxicity, and evaluate how fair the outputs of the process are based on the ground truth annotations provided by multiple judges (crowdsourcing annotators).\n",
    "\n",
    "![title](images/overview_tutorial_fairness_inclusiveness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")  \n",
    "\n",
    "import os\n",
    "\n",
    "from aif360.datasets import ToxicityDataset\n",
    "from aif360.metrics import InclusivenessLabelDatasetMetric\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as sk_met\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the toxicity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "import copy\n",
    "### The toxicity dataset (toxicity_annotations.tsv, toxicity_annotated_comments.tsv, toxicity_worker_demographics.tsv) should be downloaded from https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973\n",
    "### and placed in the folder \"data/raw/toxicity\".\n",
    "# Example on how to load the full dataset.\n",
    "#tox_dataset = ToxicityDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def clean_data(annotations, worker_demo, comments):\n",
    "        \n",
    "        # Preprocess workers.\n",
    "        worker_demo = worker_demo.replace(np.NaN, 'nan')\n",
    " \n",
    "        #### Add all the information to the annotations.\n",
    "        # Add the worker demographics.\n",
    "        annotations = annotations.reset_index().merge(worker_demo, on='worker_id', how='left').set_index(annotations.index.names)\n",
    "        # Remove the unknown demographics and the demographics with a NaN. And put them in a general test set.\n",
    "        annotations = annotations.replace(np.NaN, 'nan')\n",
    "        annotations.loc[((annotations['english_first_language'].str.contains('nan')) |(annotations['gender'].str.contains('nan')) | (annotations['age_group'].str.contains('nan')) | (annotations['education'].str.contains('nan')) ),'general_split'] = 1 #'test'\n",
    "        annotations = annotations.reset_index()\n",
    "        annotations['english_first_language'] = annotations['english_first_language'].replace('nan', 2)\n",
    "        annotations['pop_label'] = annotations[['gender', 'age_group', 'education']].apply(lambda x: ' '.join([str(x['gender']),str(x['age_group']), str(x['education'])]), axis=1)    \n",
    "          \n",
    "        # Add the comments in order to train / test the ML models.\n",
    "        annotations = annotations.reset_index().merge(comments[['comment']].reset_index(), on='rev_id', how='left').set_index('index')\n",
    "        return annotations\n",
    "    \n",
    "def normalize_text(text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        stopword_set = set(stopwords.words('english'))\n",
    "        stemmer = PorterStemmer()\n",
    "        # Convert text to lower-case and strip punctuation/symbols from words.\n",
    "        norm_text = text.lower()\n",
    "        # Replace breaks with spaces.\n",
    "        norm_text = norm_text.replace('<br />', ' ')\n",
    "        # Pad punctuation with spaces on both sides.\n",
    "        for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "            norm_text = norm_text.replace(char, ' ' + char + ' ') \n",
    "        # Tokenize.\n",
    "        norm_text = tokenizer.tokenize(norm_text)\n",
    "        # Remove stop words.\n",
    "        norm_text = [w for w in norm_text if not w in stopword_set]\n",
    "        norm_text = \" \".join(norm_text)\n",
    "        return norm_text\n",
    "\n",
    "    \n",
    "def clean_comments(comments):\n",
    "        comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "        comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "        comments['comment'] = comments['comment'].apply(lambda x: normalize_text(x))\n",
    "        return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the dataset.\n",
      "C:\\Users\\AgatheBalayn\\Documents\\thesis_related\\AIF360\\examples\\..\\aif360\\data\\raw\\toxicity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "### Read the documents    \n",
    "try:\n",
    "    print(\"Load the dataset.\")\n",
    "    filepath = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), '..', 'aif360', 'data', 'raw', 'toxicity')\n",
    "    print(filepath)\n",
    "    ### Read the documents    \n",
    "    comments = pd.read_csv(filepath + '/toxicity_annotated_comments.tsv', sep = '\\t', dtype={'rev_id':int, 'comment':str}, index_col = 0)\n",
    "    annotations = pd.read_csv(filepath + '/toxicity_annotations.tsv',  sep = '\\t', index_col=0)\n",
    "    worker_demo = pd.read_csv(filepath + '/toxicity_worker_demographics.tsv', sep='\\t')\n",
    "except IOError as err:\n",
    "    print(\"IOError: {}\".format(err))\n",
    "    print(\"To use this class, please download the following files from https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973:\")\n",
    "    print(\"\\n\\ttoxicity_annotated_comments.tsv\")\n",
    "    print(\"\\ttoxicity_annotations.tsv\")\n",
    "    print(\"\\ttoxicity_worker_demographics.tsv\")\n",
    "    print(\"\\nand place them, as-is, in the folder:\")\n",
    "    print(\"\\n\\t{}\\n\".format(os.path.abspath(os.path.join(os.path.abspath(__file__), '..', '..', 'data', 'raw', 'toxicity'))))\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "    \n",
    "### For now we do not use the whole dataset to be faster.\n",
    "n_lim = 50000\n",
    "comments = clean_comments(comments)\n",
    "annotations = annotations.head(n_lim)\n",
    "# Merge the different datasets.\n",
    "annotations = clean_data(annotations, worker_demo, comments)\n",
    "# Compute the ground truth (majority vote) label.\n",
    "annotations['MV'] = annotations.groupby(['rev_id'])['toxicity'].transform(lambda x : (x.mean() >= 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the unique comments for training and testing of the ML models.\n",
    "def prepare_aggregated_data(comment_):\n",
    "    # Get the unique comments\n",
    "    comments = comment_.drop_duplicates('rev_id')\n",
    "    # Cleaning\n",
    "    comments = comments[['comment', 'MV']]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "annotations_train, annotations_test = train_test_split(annotations, test_size=0.3)\n",
    "\n",
    "comments_train = prepare_aggregated_data(annotations_train)\n",
    "comments_test = prepare_aggregated_data(annotations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load the model.\n",
    "class DataFrameColumnExtracter_doc(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.9597758655193116\n",
      "Test accuracy:  0.960602310231023\n",
      "Training confusion matrix: [[0.95954357 0.03389831]\n",
      " [0.04045643 0.96610169]]\n",
      "Test confusion matrix: [[0.9602139  0.02890173]\n",
      " [0.0397861  0.97109827]]\n"
     ]
    }
   ],
   "source": [
    "### The ML model is then trained on the majority vote labels. \n",
    "\n",
    "## 1) Perform grid search over the parameters of the model.\n",
    "\n",
    "# Load the model, here Logistic Regression model.\n",
    "clf_LR = Pipeline([# Sentences.\n",
    "                  ('sentences_features', Pipeline([\n",
    "                      ('sentence_extractor', DataFrameColumnExtracter_doc('comment')),#.values.astype('U'),\n",
    "                    ('vect', CountVectorizer(max_features = 1500, ngram_range = (1,5), analyzer = 'char')),\n",
    "                     ('tf', TfidfTransformer(norm = 'l2'))\n",
    "                  ])),\n",
    "            # Classifier.\n",
    "            ('clf', LogisticRegression())#C=LR_C, tol=LR_C_tol))\n",
    "        ])\n",
    "\n",
    "# Parameters of the grid search:\n",
    "tuned_parameters = {'clf__C': [1e-4, 1e-2, 1, 10], 'clf__tol': [1, 1e-2, 1e-4]} \n",
    "\n",
    "# Initialize the grid search.\n",
    "clf = GridSearchCV(clf_LR, tuned_parameters, cv=5, verbose=0)\n",
    "\n",
    "# Functions to save and load the results of the grid search.\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "# Use a small number of data to train the model faster.\n",
    "nb_data = 5000\n",
    "\n",
    "# Train the grid search.\n",
    "# To comment if already run ones and the paramters were saved.\n",
    "# =============\n",
    "#best_model = clf.fit(dataset_train_comments[0:nb_data], dataset_train_comments['MV'][0:nb_data])\n",
    "#best_parameters = best_model.best_params_  \n",
    "#print(best_parameters)  \n",
    "#best_result = best_model.best_score_  \n",
    "#print(best_result)  \n",
    "#save_obj(best_parameters, 'best_param_LR_aggregated')\n",
    "# =============\n",
    "\n",
    "\n",
    "## 2) Train the final model.\n",
    "best_parameters = load_obj('best_param_LR_aggregated')\n",
    "clf_LR.set_params(**best_parameters)\n",
    "clf_LR.fit(comments_train[0:nb_data], comments_train['MV'][0:nb_data])\n",
    "\n",
    "\n",
    "## 3) Evaluate general performance.\n",
    "train_pred = clf_LR.predict(comments_train[0:nb_data])\n",
    "test_pred = clf_LR.predict(comments_test[0:nb_data])\n",
    "print(\"Training accuracy: \", sk_met.accuracy_score(comments_train['MV'][0:nb_data], train_pred))\n",
    "print(\"Test accuracy: \", sk_met.accuracy_score(comments_test['MV'][0:nb_data], test_pred))\n",
    "\n",
    "C_train = sk_met.confusion_matrix(comments_train['MV'][0:nb_data], train_pred)\n",
    "C_train = C_train / C_train.astype(np.float).sum(axis=0)\n",
    "C_test = sk_met.confusion_matrix(comments_test['MV'][0:nb_data], test_pred)\n",
    "C_test = C_test / C_test.astype(np.float).sum(axis=0)\n",
    "print(\"Training confusion matrix:\", C_train)\n",
    "print(\"Test confusion matrix:\", C_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the fairness performance (both on training and test datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import SubjectivityDataset\n",
    "\n",
    "def default_preprocessing(df):\n",
    "    return df\n",
    "\n",
    "# Wrapper to load the datasets to compute the fairness on.\n",
    "def subjectivity_dataset_wrapper(annotations, label_name, #favorable_classes,\n",
    "                 protected_attribute_names=['gender', 'english_first_language', 'age_group', 'education', 'rev_id', 'worker_id', 'pop_label'], privileged_classes=None,\n",
    "                 instance_weights_name=None, categorical_features=[],\n",
    "                 features_to_keep=[], features_to_drop=[], na_values=[],\n",
    "                 custom_preprocessing=default_preprocessing, \n",
    "                metadata={'label_maps': [{1.0: 'Toxic', 0.0: 'Non-toxic'}],},\n",
    "                mapping_categorical_protected=(('gender',('female','male', 'other', 'nan')), ('age_group',('Under 18', '18-30', '30-45', '45-60', 'Over 60', 'nan')), ('education',('none', 'hs', 'some', 'bachelors', 'masters', 'professional', 'doctorate', 'nan')))):\n",
    "    \n",
    "    if 'comment' in annotations.columns.tolist():\n",
    "        annotations = annotations.drop('comment', axis=1)\n",
    "    if 'general_split' in annotations.columns.tolist():\n",
    "        annotations = annotations.drop('general_split', axis=1)\n",
    "                \n",
    "    if label_name != 'toxicity':\n",
    "        # Delete the 'toxicity' column.\n",
    "        if 'toxicity' in annotations.columns.tolist():\n",
    "            annotations = annotations.drop('toxicity', axis=1)\n",
    "        annotations.rename(columns={label_name:'toxicity'}, inplace=True)\n",
    "        label_name = 'toxicity'\n",
    "    else:\n",
    "        if 'pred_1' in annotations.columns.tolist():\n",
    "            annotations = annotations.drop('pred_1', axis=1)\n",
    "    \n",
    "    # Make the categorical data numbers.\n",
    "    if mapping_categorical_protected != ():\n",
    "        for tuple_type in mapping_categorical_protected:\n",
    "            for tuple_details in tuple_type:\n",
    "                if tuple_type.index(tuple_details) == 0:\n",
    "                    key = tuple_details\n",
    "                else:\n",
    "                    for tuple_categories in tuple_details:\n",
    "                        annotations[key] = annotations[key].replace(tuple_categories, tuple_details.index(tuple_categories))\n",
    "\n",
    "    annotations['pop_label'] = annotations[['gender', 'age_group', 'education']].apply(lambda x: int(''.join([str(x['gender']),str(x['age_group']), str(x['education'])])), axis=1)    \n",
    "    annotations = annotations[['rev_id', 'worker_id', 'toxicity', 'toxicity_score', 'gender', 'english_first_language', 'age_group', 'education', 'pop_label', 'MV']]\n",
    "    \n",
    "    dataset = SubjectivityDataset(annotations, label_name,\n",
    "                 protected_attribute_names=protected_attribute_names, privileged_classes=privileged_classes,\n",
    "                 instance_weights_name=instance_weights_name, categorical_features=categorical_features,\n",
    "                 features_to_keep=features_to_keep, features_to_drop=features_to_drop, na_values=na_values,\n",
    "                 custom_preprocessing=custom_preprocessing, metadata=metadata)\n",
    "    return dataset, annotations\n",
    "\n",
    "\n",
    "# The dataset might be too large for the classifier to process all the data. In this case, it is splitted to get the predictions on all the data.\n",
    "def compute_pred_dataset(dataset_orig_train, prediction_col, nb_data):\n",
    "    for i in range(int(len(dataset_orig_train) / nb_data)):\n",
    "        print(i)\n",
    "        low_interval = i*nb_data\n",
    "        high_interval = (i+1)*nb_data\n",
    "        dataset_orig_train[prediction_col].iloc[low_interval:high_interval] = clf_LR.predict(dataset_orig_train.iloc[low_interval:high_interval])\n",
    "    if high_interval < len(dataset_orig_train):\n",
    "        dataset_orig_train[prediction_col].iloc[high_interval:] = clf_LR.predict(dataset_orig_train.iloc[high_interval:])\n",
    "    return dataset_orig_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "### Create SubjectivityDataset datasets out of the training and test results and ground truth.\n",
    "\n",
    "prediction_col = \"pred_1\"\n",
    "annotations_train[prediction_col] = -1\n",
    "annotations_test[prediction_col] = -1\n",
    "nb_data = 5000\n",
    "\n",
    "#annotations_train = compute_pred_dataset(annotations_train, prediction_col, nb_data)\n",
    "#subjectivity_dataset_train_GT = subjectivity_dataset_wrapper(annotations_train, 'toxicity')\n",
    "#subjectivity_dataset_train_outputs = subjectivity_dataset_wrapper(annotations_train, prediction_col)\n",
    "subjectivity_dataset_test_GT, annotations_subjectivity_dataset_test_GT = subjectivity_dataset_wrapper(annotations_test, 'toxicity')\n",
    "annotations_test_pred = compute_pred_dataset(annotations_test, prediction_col, nb_data)\n",
    "subjectivity_dataset_test_outputs = copy.deepcopy(subjectivity_dataset_test_GT)\n",
    "subjectivity_dataset_test_outputs.labels = annotations_test_pred[prediction_col].values.copy()\n",
    "subjectivity_dataset_test_outputs.scores = subjectivity_dataset_test_outputs.labels\n",
    "#subjectivity_dataset_test_outputs, annotations_subjectivity_dataset_test_outputs = subjectivity_dataset_wrapper(annotations_test_pred, prediction_col)\n",
    "\n",
    "# Instantiate the fairness metric class.\n",
    "test_metric_inclusion = InclusivenessLabelDatasetMetric(subjectivity_dataset_test_GT, subjectivity_dataset_test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aif360\\metrics\\classification_metric.py:270: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "..\\aif360\\metrics\\classification_metric.py:271: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:151: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  matrix_data = data.as_matrix([metric_to_compute])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.7787828338336237, 0.6250096763107369,                accuracy\n",
       "  bin_col                \n",
       "  (-0.001, 0.2]  0.945116\n",
       "  (0.2, 0.4]     0.772582\n",
       "  (0.4, 0.6]     0.643733\n",
       "  (0.6, 0.8]     0.351852\n",
       "  (0.8, 1.0]     0.411765)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metric_inclusion.compute_bin_metrics('annotator_disagreement', ('accuracy',), number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = test_metric_inclusion.compute_bin_metrics('annotation_popularity', ('accuracy','TPR', 'TNR', 'FPR', 'FNR'), number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aif360\\metrics\\classification_metric.py:270: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "..\\aif360\\metrics\\classification_metric.py:271: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n"
     ]
    }
   ],
   "source": [
    "results_test_demography = test_metric_inclusion.compute_bin_metrics('demography', ('accuracy', 'TPR'), number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aif360\\metrics\\classification_metric.py:270: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "..\\aif360\\metrics\\classification_metric.py:271: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  bin_values[metric_to_compute] = list_bin_metric[eval_metrics.index(metric_to_compute)]\n",
      "..\\aif360\\metrics\\inclusiveness_label_dataset_metric.py:151: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  matrix_data = data.as_matrix([metric_to_compute])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.8287835607161211, 0.7231189301391857,             accuracy\n",
       "  bin_col             \n",
       "  (0.5, 0.6]  0.503854\n",
       "  (0.6, 0.7]  0.573677\n",
       "  (0.7, 0.8]  0.724600\n",
       "  (0.8, 0.9]  0.840643\n",
       "  (0.9, 1.0]  0.972821)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metric_inclusion.compute_bin_metrics('sample_ambiguity', ('accuracy',), number_bins=None, filtering_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_demography_index(data):\n",
    "    for index, row in data.iterrows():\n",
    "        print(index)\n",
    "        print(str(int(index)))\n",
    "        demog_translation = ''\n",
    "         \n",
    "        # Find the fist element (gender)\n",
    "        if len(str(int(index))) < 3:\n",
    "            # This means the gender is 0.\n",
    "            demog_translation += 'female'\n",
    "        else:\n",
    "            if str(int(index))[0] == '1':\n",
    "                demog_translation += 'male'\n",
    "            elif str(int(index))[0] == '2':\n",
    "                demog_translation += 'other'\n",
    "            elif str(int(index))[0] == '3':\n",
    "                demog_translation += 'nan'\n",
    "        # Find the second element:\n",
    "        demog_translation += ' '\n",
    "        if len(str(int(index))) < 2:\n",
    "            # This means the gender and age is 0.\n",
    "            demog_translation += 'Under 18'\n",
    "        else:\n",
    "            if str(int(index))[1] == '1':\n",
    "                demog_translation += '18-30'\n",
    "            elif str(int(index))[1] == '2':\n",
    "                demog_translation += '30-45'\n",
    "            elif str(int(index))[1] == '3':\n",
    "                demog_translation += '45-60'\n",
    "            elif str(int(index))[1] == '4':\n",
    "                demog_translation += 'Over 60'\n",
    "            elif str(int(index))[1] == '5':\n",
    "                demog_translation += 'nan'\n",
    "    \n",
    "        # Find the third element:\n",
    "        demog_translation += ' '\n",
    "        index_element = len(str(int(index))) - 1\n",
    "        \n",
    "        if str(int(index))[index_element] == '0':\n",
    "            demog_translation += 'none'\n",
    "        elif str(int(index))[index_element] == '1':\n",
    "            demog_translation += 'hs'\n",
    "        elif str(int(index))[index_element] == '2':\n",
    "            demog_translation += 'some'\n",
    "        elif str(int(index))[index_element] == '3':\n",
    "            demog_translation += 'bachelors'\n",
    "        elif str(int(index))[index_element] == '4':\n",
    "            demog_translation += 'masters'\n",
    "        elif str(int(index))[index_element] == '5':\n",
    "            demog_translation += 'professional'\n",
    "        elif str(int(index))[index_element] == '6':\n",
    "            demog_translation += 'doctorate'\n",
    "        elif str(int(index))[index_element] == '7':\n",
    "            demog_translation += 'nan'\n",
    "        data.rename(index={index:demog_translation}, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114.0\n",
      "114\n",
      "25.0\n",
      "25\n",
      "123.0\n",
      "123\n",
      "113.0\n",
      "113\n",
      "24.0\n",
      "24\n",
      "23.0\n",
      "23\n",
      "21.0\n",
      "21\n",
      "14.0\n",
      "14\n",
      "11.0\n",
      "11\n",
      "357.0\n",
      "357\n",
      "124.0\n",
      "124\n",
      "53.0\n",
      "53\n",
      "13.0\n",
      "13\n",
      "134.0\n",
      "134\n",
      "121.0\n",
      "121\n",
      "125.0\n",
      "125\n",
      "15.0\n",
      "15\n",
      "103.0\n",
      "103\n",
      "115.0\n",
      "115\n",
      "12.0\n",
      "12\n",
      "33.0\n",
      "33\n",
      "133.0\n",
      "133\n",
      "111.0\n",
      "111\n",
      "116.0\n",
      "116\n",
      "101.0\n",
      "101\n",
      "131.0\n",
      "131\n",
      "135.0\n",
      "135\n",
      "31.0\n",
      "31\n",
      "54.0\n",
      "54\n",
      "35.0\n",
      "35\n",
      "2.0\n",
      "2\n",
      "34.0\n",
      "34\n",
      "153.0\n",
      "153\n",
      "45.0\n",
      "45\n",
      "112.0\n",
      "112\n",
      "3.0\n",
      "3\n",
      "102.0\n",
      "102\n",
      "1.0\n",
      "1\n",
      "122.0\n",
      "122\n",
      "110.0\n",
      "110\n",
      "43.0\n",
      "43\n",
      "51.0\n",
      "51\n",
      "26.0\n",
      "26\n",
      "151.0\n",
      "151\n",
      "52.0\n",
      "52\n",
      "132.0\n",
      "132\n",
      "22.0\n",
      "22\n",
      "44.0\n",
      "44\n",
      "155.0\n",
      "155\n",
      "154.0\n",
      "154\n",
      "16.0\n",
      "16\n",
      "126.0\n",
      "126\n",
      "136.0\n",
      "136\n",
      "104.0\n",
      "104\n",
      "32.0\n",
      "32\n",
      "42.0\n",
      "42\n",
      "143.0\n",
      "143\n",
      "41.0\n",
      "41\n",
      "152.0\n",
      "152\n",
      "145.0\n",
      "145\n",
      "46.0\n",
      "46\n",
      "201.0\n",
      "201\n",
      "56.0\n",
      "56\n",
      "36.0\n",
      "36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pop_label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>male 18-30 masters</th>\n",
       "      <td>0.930712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female nan professional</th>\n",
       "      <td>0.910053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 30-45 bachelors</th>\n",
       "      <td>0.924272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 18-30 bachelors</th>\n",
       "      <td>0.926213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female Over 60 masters</th>\n",
       "      <td>0.866953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 45-60 bachelors</th>\n",
       "      <td>0.900585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 18-30 hs</th>\n",
       "      <td>0.914948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female Over 60 masters</th>\n",
       "      <td>0.907937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 18-30 hs</th>\n",
       "      <td>0.914729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nan nan nan</th>\n",
       "      <td>0.921830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 30-45 masters</th>\n",
       "      <td>0.926496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 45-60 bachelors</th>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 45-60 bachelors</th>\n",
       "      <td>0.903259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 45-60 masters</th>\n",
       "      <td>0.897196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 30-45 hs</th>\n",
       "      <td>0.923599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 30-45 professional</th>\n",
       "      <td>0.910417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female nan professional</th>\n",
       "      <td>0.905473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male  bachelors</th>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 18-30 professional</th>\n",
       "      <td>0.928832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 30-45 some</th>\n",
       "      <td>0.945946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 45-60 bachelors</th>\n",
       "      <td>0.959459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 45-60 bachelors</th>\n",
       "      <td>0.917910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 18-30 hs</th>\n",
       "      <td>0.916552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 18-30 doctorate</th>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male  hs</th>\n",
       "      <td>0.945736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 45-60 hs</th>\n",
       "      <td>0.944828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 45-60 professional</th>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 18-30 hs</th>\n",
       "      <td>0.897143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female Over 60 masters</th>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female nan professional</th>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 18-30 some</th>\n",
       "      <td>0.902655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female Under 18 bachelors</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male  some</th>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female Under 18 hs</th>\n",
       "      <td>0.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 30-45 some</th>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 18-30 none</th>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 45-60 bachelors</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 18-30 hs</th>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female  doctorate</th>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male nan hs</th>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 30-45 some</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 45-60 some</th>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 30-45 some</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female Over 60 masters</th>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male nan professional</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male nan masters</th>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female  doctorate</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 30-45 doctorate</th>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male 45-60 doctorate</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male  masters</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 30-45 some</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 30-45 some</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male Over 60 bachelors</th>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female 18-30 hs</th>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male nan some</th>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male Over 60 professional</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female  doctorate</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other  hs</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female  doctorate</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female  doctorate</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy\n",
       "pop_label                          \n",
       "male 18-30 masters         0.930712\n",
       "female nan professional    0.910053\n",
       "male 30-45 bachelors       0.924272\n",
       "male 18-30 bachelors       0.926213\n",
       "female Over 60 masters     0.866953\n",
       "female 45-60 bachelors     0.900585\n",
       "female 18-30 hs            0.914948\n",
       "female Over 60 masters     0.907937\n",
       "female 18-30 hs            0.914729\n",
       "nan nan nan                0.921830\n",
       "male 30-45 masters         0.926496\n",
       "female 45-60 bachelors     0.937500\n",
       "female 45-60 bachelors     0.903259\n",
       "male 45-60 masters         0.897196\n",
       "male 30-45 hs              0.923599\n",
       "male 30-45 professional    0.910417\n",
       "female nan professional    0.905473\n",
       "male  bachelors            0.903226\n",
       "male 18-30 professional    0.928832\n",
       "female 30-45 some          0.945946\n",
       "female 45-60 bachelors     0.959459\n",
       "male 45-60 bachelors       0.917910\n",
       "male 18-30 hs              0.916552\n",
       "male 18-30 doctorate       0.818182\n",
       "male  hs                   0.945736\n",
       "male 45-60 hs              0.944828\n",
       "male 45-60 professional    0.920000\n",
       "female 18-30 hs            0.897143\n",
       "female Over 60 masters     0.941176\n",
       "female nan professional    0.941176\n",
       "...                             ...\n",
       "male 18-30 some            0.902655\n",
       "female Under 18 bachelors  1.000000\n",
       "male  some                 0.880000\n",
       "female Under 18 hs         0.851852\n",
       "male 30-45 some            0.951220\n",
       "male 18-30 none            0.888889\n",
       "female 45-60 bachelors     0.750000\n",
       "female 18-30 hs            0.941176\n",
       "female  doctorate          0.900000\n",
       "male nan hs                0.960000\n",
       "female 30-45 some          1.000000\n",
       "male 45-60 some            0.875000\n",
       "female 30-45 some          1.000000\n",
       "female Over 60 masters     0.714286\n",
       "male nan professional      1.000000\n",
       "male nan masters           0.870968\n",
       "female  doctorate          1.000000\n",
       "male 30-45 doctorate       0.947368\n",
       "male 45-60 doctorate       1.000000\n",
       "male  masters              1.000000\n",
       "female 30-45 some          1.000000\n",
       "female 30-45 some          1.000000\n",
       "male Over 60 bachelors     0.642857\n",
       "female 18-30 hs            0.933333\n",
       "male nan some              0.857143\n",
       "male Over 60 professional  1.000000\n",
       "female  doctorate          1.000000\n",
       "other  hs                  1.000000\n",
       "female  doctorate          1.000000\n",
       "female  doctorate          1.000000\n",
       "\n",
       "[64 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_demography_index(results_test_demography[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_plot_color(value):\n",
    "    if value > 0.5:\n",
    "        return 'b'\n",
    "    else:\n",
    "        return 'w'\n",
    "\n",
    "def plot_fairness(data, metric, bin_name=''):\n",
    "    yticks = list(data.index.values)\n",
    "\n",
    "    # Plot the bins\n",
    "    data = data.as_matrix(columns=[metric])\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(data)\n",
    "    cbar = fig.colorbar(heatmap, ax=ax)\n",
    "    cbar.set_label(metric, rotation=90)\n",
    "\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_yticklabels(yticks,rotation=0)\n",
    "    ax.set_ylabel(bin_name)\n",
    "    ax.tick_params(left='off', bottom='off',labelbottom='off', color='grey',labelsize='small')\n",
    "\n",
    "    # Add the exact evaluation measure per bin\n",
    "    for i in range(data.shape[0]):\n",
    "        text = ax.text(0.5, i+0.5, np.round(data[i][0], 2), ha=\"center\", va=\"center\", color=annotation_plot_color(data[i][0]))\n",
    "\n",
    "    fig.set_figwidth(4)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\agathebalayn\\documents\\thesis_related\\virtualenvs\\aif306_v2\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_fairness(results_test[0][2], 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
